{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>labels</th>\n",
       "      <th>cmeclose</th>\n",
       "      <th>cmcclose</th>\n",
       "      <th>google</th>\n",
       "      <th>ravenpack</th>\n",
       "      <th>XAUprice</th>\n",
       "      <th>cmcvolume</th>\n",
       "      <th>market</th>\n",
       "      <th>AveBlockSize</th>\n",
       "      <th>...</th>\n",
       "      <th>CostPerTran</th>\n",
       "      <th>FeePerTran</th>\n",
       "      <th>MinRevenue</th>\n",
       "      <th>NetDifficulty</th>\n",
       "      <th>TolHashRate</th>\n",
       "      <th>TolTranFee</th>\n",
       "      <th>ConTranPerDay</th>\n",
       "      <th>EstTranValue</th>\n",
       "      <th>OutValue</th>\n",
       "      <th>UniAddressUsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>-1</td>\n",
       "      <td>17545</td>\n",
       "      <td>17776.7</td>\n",
       "      <td>52</td>\n",
       "      <td>0.24737</td>\n",
       "      <td>1261.72</td>\n",
       "      <td>16894499840</td>\n",
       "      <td>3.162220e+11</td>\n",
       "      <td>1.060159</td>\n",
       "      <td>...</td>\n",
       "      <td>116.815051</td>\n",
       "      <td>32.003749</td>\n",
       "      <td>44228748.17</td>\n",
       "      <td>1873110000000</td>\n",
       "      <td>13035760.95</td>\n",
       "      <td>12117323.34</td>\n",
       "      <td>378622</td>\n",
       "      <td>3.906879e+09</td>\n",
       "      <td>1984897.730</td>\n",
       "      <td>887698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>-1</td>\n",
       "      <td>16490</td>\n",
       "      <td>16624.6</td>\n",
       "      <td>69</td>\n",
       "      <td>0.23034</td>\n",
       "      <td>1265.62</td>\n",
       "      <td>22149699584</td>\n",
       "      <td>2.775528e+11</td>\n",
       "      <td>1.072800</td>\n",
       "      <td>...</td>\n",
       "      <td>119.379078</td>\n",
       "      <td>41.644261</td>\n",
       "      <td>44203326.97</td>\n",
       "      <td>1873110000000</td>\n",
       "      <td>12663310.64</td>\n",
       "      <td>15419911.88</td>\n",
       "      <td>370277</td>\n",
       "      <td>5.449321e+09</td>\n",
       "      <td>2227875.178</td>\n",
       "      <td>853864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>-1</td>\n",
       "      <td>15555</td>\n",
       "      <td>15802.9</td>\n",
       "      <td>54</td>\n",
       "      <td>0.17781</td>\n",
       "      <td>1266.76</td>\n",
       "      <td>16516599808</td>\n",
       "      <td>2.781525e+11</td>\n",
       "      <td>1.038692</td>\n",
       "      <td>...</td>\n",
       "      <td>135.173009</td>\n",
       "      <td>54.790393</td>\n",
       "      <td>46165232.30</td>\n",
       "      <td>1873110000000</td>\n",
       "      <td>12290860.32</td>\n",
       "      <td>18712398.65</td>\n",
       "      <td>335482</td>\n",
       "      <td>3.765982e+09</td>\n",
       "      <td>1791086.429</td>\n",
       "      <td>840740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>-1</td>\n",
       "      <td>15555</td>\n",
       "      <td>13831.8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.13745</td>\n",
       "      <td>1275.01</td>\n",
       "      <td>22197999616</td>\n",
       "      <td>2.602869e+11</td>\n",
       "      <td>1.062319</td>\n",
       "      <td>...</td>\n",
       "      <td>125.710492</td>\n",
       "      <td>54.637979</td>\n",
       "      <td>48973790.42</td>\n",
       "      <td>1873110000000</td>\n",
       "      <td>14432449.62</td>\n",
       "      <td>21285645.39</td>\n",
       "      <td>380648</td>\n",
       "      <td>4.135072e+09</td>\n",
       "      <td>1901698.950</td>\n",
       "      <td>890732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-23</td>\n",
       "      <td>1</td>\n",
       "      <td>15555</td>\n",
       "      <td>14699.2</td>\n",
       "      <td>63</td>\n",
       "      <td>0.08594</td>\n",
       "      <td>1275.01</td>\n",
       "      <td>13086000128</td>\n",
       "      <td>2.332497e+11</td>\n",
       "      <td>1.060038</td>\n",
       "      <td>...</td>\n",
       "      <td>135.118563</td>\n",
       "      <td>51.402623</td>\n",
       "      <td>41645027.49</td>\n",
       "      <td>1873110000000</td>\n",
       "      <td>12942648.37</td>\n",
       "      <td>15842853.90</td>\n",
       "      <td>308211</td>\n",
       "      <td>2.381108e+09</td>\n",
       "      <td>1781229.264</td>\n",
       "      <td>729638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  labels  cmeclose  cmcclose  google  ravenpack  XAUprice  \\\n",
       "0  2017-12-19      -1     17545   17776.7      52    0.24737   1261.72   \n",
       "1  2017-12-20      -1     16490   16624.6      69    0.23034   1265.62   \n",
       "2  2017-12-21      -1     15555   15802.9      54    0.17781   1266.76   \n",
       "3  2017-12-22      -1     15555   13831.8     100    0.13745   1275.01   \n",
       "4  2017-12-23       1     15555   14699.2      63    0.08594   1275.01   \n",
       "\n",
       "     cmcvolume        market  AveBlockSize  ...  CostPerTran  FeePerTran  \\\n",
       "0  16894499840  3.162220e+11      1.060159  ...   116.815051   32.003749   \n",
       "1  22149699584  2.775528e+11      1.072800  ...   119.379078   41.644261   \n",
       "2  16516599808  2.781525e+11      1.038692  ...   135.173009   54.790393   \n",
       "3  22197999616  2.602869e+11      1.062319  ...   125.710492   54.637979   \n",
       "4  13086000128  2.332497e+11      1.060038  ...   135.118563   51.402623   \n",
       "\n",
       "    MinRevenue  NetDifficulty  TolHashRate   TolTranFee  ConTranPerDay  \\\n",
       "0  44228748.17  1873110000000  13035760.95  12117323.34         378622   \n",
       "1  44203326.97  1873110000000  12663310.64  15419911.88         370277   \n",
       "2  46165232.30  1873110000000  12290860.32  18712398.65         335482   \n",
       "3  48973790.42  1873110000000  14432449.62  21285645.39         380648   \n",
       "4  41645027.49  1873110000000  12942648.37  15842853.90         308211   \n",
       "\n",
       "   EstTranValue     OutValue  UniAddressUsed  \n",
       "0  3.906879e+09  1984897.730          887698  \n",
       "1  5.449321e+09  2227875.178          853864  \n",
       "2  3.765982e+09  1791086.429          840740  \n",
       "3  4.135072e+09  1901698.950          890732  \n",
       "4  2.381108e+09  1781229.264          729638  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "dataframe1 = read_csv('DailyData1.csv', engine='python')\n",
    "dataframe = dataframe1.iloc[:,2:]\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float64')\n",
    "dataframe1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataframe1.iloc[:,1:2].values\n",
    "scaler1 = OneHotEncoder()\n",
    "labels = scaler1.fit_transform(labels)\n",
    "\n",
    "labels=labels.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset,dataset1,look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(0,len(dataset)-look_back,1):\n",
    "        a1 = dataset[i:(i+look_back),:]\n",
    "        a2 = dataset1[i:(i+look_back),:]\n",
    "        a = numpy.hstack((a1,a2))\n",
    "        dataX.append(a)\n",
    "        b = dataset1[(i+look_back):(i+look_back+1),1]\n",
    "        dataY.append(b)\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back = 20\n",
    "look_lag = 1 #用前20个数据预测当前1个数据\n",
    "train_size = 626\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[:train_size,:], dataset[train_size-look_back:len(dataset),:]\n",
    "train1, test1 = labels[:train_size,:], labels[train_size-look_back:len(dataset),:]\n",
    "trainX, trainY = create_dataset(train,train1,look_back)\n",
    "testX, testY = create_dataset(test, test1 ,look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], look_back,25))\n",
    "testX = numpy.reshape(testX, (testX.shape[0],look_back,25))\n",
    "\n",
    "features_set = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1]*25))\n",
    "scaler1 = StandardScaler().fit(features_set) #MinMaxScaler(feature_range=(-1, 1))\n",
    "features_set = scaler1.fit_transform(features_set)\n",
    "#pca=PCA(n_components=500).fit(features_set)\n",
    "#features_set = pca.transform(features_set)\n",
    "test_features = numpy.reshape(testX, (testX.shape[0], testX.shape[1]*25))\n",
    "test_features = scaler1.fit_transform(test_features)\n",
    "#test_features = pca.transform(test_features)\n",
    "\n",
    "def result(testY,testPredict):\n",
    "    from sklearn.metrics import accuracy_score,classification_report\n",
    "    acc = accuracy_score(testY,testPredict)\n",
    "    report = classification_report(testY,testPredict)\n",
    "    print(\"acc:\",acc)\n",
    "    print(report)\n",
    "\n",
    "trainY = trainY.reshape(trainY.shape[0])\n",
    "testY = testY.reshape(testY.shape[0])\n",
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model1 = LR() #C=100,max_iter=5000\n",
    "model1.fit(features_set, trainY)\n",
    "testPredict1 = model1.predict(test_features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5432692307692307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.52      0.56       115\n",
      "         1.0       0.49      0.57      0.53        93\n",
      "\n",
      "    accuracy                           0.54       208\n",
      "   macro avg       0.55      0.55      0.54       208\n",
      "weighted avg       0.55      0.54      0.54       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result(testY,testPredict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5192307692307693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.52      0.55       115\n",
      "         1.0       0.47      0.52      0.49        93\n",
      "\n",
      "    accuracy                           0.52       208\n",
      "   macro avg       0.52      0.52      0.52       208\n",
      "weighted avg       0.52      0.52      0.52       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model2 = LinearDiscriminantAnalysis() \n",
    "model2.fit(features_set, trainY)\n",
    "testPredict2 = model2.predict(test_features)  \n",
    "result(testY,testPredict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5528846153846154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.57      0.58       115\n",
      "         1.0       0.50      0.54      0.52        93\n",
      "\n",
      "    accuracy                           0.55       208\n",
      "   macro avg       0.55      0.55      0.55       208\n",
      "weighted avg       0.56      0.55      0.55       208\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model3 = QuadraticDiscriminantAnalysis() \n",
    "model3.fit(features_set, trainY)\n",
    "testPredict3 = model3.predict(test_features)  \n",
    "result(testY,testPredict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: xgboost in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from xgboost) (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5528846153846154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.55      0.58       115\n",
      "         1.0       0.50      0.56      0.53        93\n",
      "\n",
      "    accuracy                           0.55       208\n",
      "   macro avg       0.55      0.55      0.55       208\n",
      "weighted avg       0.56      0.55      0.55       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model4 = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.5, learning_rate = 0.003,\n",
    "                max_depth = 20, alpha = 20, n_estimators = 400)\n",
    "'''\n",
    "objective ='reg:logistic', colsample_bytree = 0.5, learning_rate = 0.003,\n",
    "                max_depth = 20, alpha = 20, n_estimators = 400\n",
    "'''\n",
    "model4.fit(features_set, trainY)\n",
    "testPredict4 = model4.predict(test_features)  \n",
    "result(testY,testPredict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5480769230769231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.50      0.55       115\n",
      "         1.0       0.50      0.60      0.54        93\n",
      "\n",
      "    accuracy                           0.55       208\n",
      "   macro avg       0.55      0.55      0.55       208\n",
      "weighted avg       0.56      0.55      0.55       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model5 = svm.SVC()\n",
    "model5.fit(features_set, trainY)\n",
    "testPredict5 = model5.predict(test_features)  \n",
    "result(testY,testPredict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier()#n_estimators=400,criterion=\"entropy\"\n",
    "regr.fit(features_set, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5288461538461539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.47      0.52       115\n",
      "         1.0       0.48      0.60      0.53        93\n",
      "\n",
      "    accuracy                           0.53       208\n",
      "   macro avg       0.54      0.54      0.53       208\n",
      "weighted avg       0.54      0.53      0.53       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testPredict6 = regr.predict(test_features)  \n",
    "result(testY,testPredict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras==2.2.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 750 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 37.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/envs/python3/lib/python3.6/site-packages (from keras==2.2.4) (1.15.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 43.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/python3/lib/python3.6/site-packages (from keras==2.2.4) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python3/lib/python3.6/site-packages (from keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/python3/lib/python3.6/site-packages (from keras==2.2.4) (1.5.0)\n",
      "Collecting h5py\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: keras-preprocessing, h5py, keras-applications, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5376850f2dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Conv1D,Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "%matplotlib inline\n",
    "from keras import optimizers,initializers\n",
    "model = Sequential() \n",
    "\n",
    "model.add(LSTM(units=96, return_sequences=True, \n",
    "               input_shape=(trainX.shape[1],trainX.shape[2])))#\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=48, return_sequences=True,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(units=48,return_sequences=False,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "'''\n",
    "model.add(Conv1D(kernel_size=20, filters=3,activation='relu'))\n",
    "model.add(Flatten())\n",
    "'''\n",
    "\n",
    "#model.add(Dense(units = 16,activation='relu'))\n",
    "model.add(Dense(units = 1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',#binary_crossentropy\n",
    "                  optimizer='adam',#RMSprop optimizers.Adam(lr=0.00001)\n",
    "                  metrics=['accuracy'])  #mean_squared_error metrics=['accuracy']\n",
    "history = model.fit(trainX,trainY,epochs=500, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5144230769230769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.42      0.49       115\n",
      "         1.0       0.47      0.63      0.54        93\n",
      "\n",
      "    accuracy                           0.51       208\n",
      "   macro avg       0.53      0.53      0.51       208\n",
      "weighted avg       0.53      0.51      0.51       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testPredict = model.predict_classes(testX)\n",
    "result(testY,testPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
