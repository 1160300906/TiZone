{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>BBANDS_upper</th>\n",
       "      <th>BBANDS_middle</th>\n",
       "      <th>BBANDS_lower</th>\n",
       "      <th>EMA</th>\n",
       "      <th>MA</th>\n",
       "      <th>SMA</th>\n",
       "      <th>RSI</th>\n",
       "      <th>TRIX</th>\n",
       "      <th>ROC</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdsignal</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ATR</th>\n",
       "      <th>CCI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4269.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4269.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4252.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4251.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4246.92</td>\n",
       "      <td>4273.626544</td>\n",
       "      <td>4263.478797</td>\n",
       "      <td>4253.331049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     close  BBANDS_upper  BBANDS_middle  BBANDS_lower  EMA  MA  SMA  RSI  \\\n",
       "0  4269.26           NaN            NaN           NaN  NaN NaN  NaN  NaN   \n",
       "1  4269.01           NaN            NaN           NaN  NaN NaN  NaN  NaN   \n",
       "2  4252.01           NaN            NaN           NaN  NaN NaN  NaN  NaN   \n",
       "3  4251.67           NaN            NaN           NaN  NaN NaN  NaN  NaN   \n",
       "4  4246.92   4273.626544    4263.478797   4253.331049  NaN NaN  NaN  NaN   \n",
       "\n",
       "   TRIX  ROC  macd  macdsignal  macdhist  ADX  ATR  CCI  \n",
       "0   NaN  NaN   NaN         NaN       NaN  NaN  NaN  NaN  \n",
       "1   NaN  NaN   NaN         NaN       NaN  NaN  NaN  NaN  \n",
       "2   NaN  NaN   NaN         NaN       NaN  NaN  NaN  NaN  \n",
       "3   NaN  NaN   NaN         NaN       NaN  NaN  NaN  NaN  \n",
       "4   NaN  NaN   NaN         NaN       NaN  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "dataframe1 = read_csv('newBinanceData1.csv', engine='python')\n",
    "dataframe = dataframe1[['close','BBANDS_upper', 'BBANDS_middle', 'BBANDS_lower','EMA','MA','SMA','RSI','TRIX','ROC','macd'\n",
    "                                          , 'macdsignal', 'macdhist','ADX','ATR','CCI']]\n",
    "dataset = dataframe.iloc[88:,:].values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float64')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back,look_lag,temp):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(0,len(dataset)-look_back-look_lag+1,temp):\n",
    "        a = dataset[i:(i+look_back), 1:]\n",
    "        dataX.append(a)\n",
    "        b1 = dataset[(i+look_back-1):(i+look_back),0]\n",
    "        b2 = dataset[(i+look_back):(i+look_back+look_lag),0]\n",
    "        if (b2*1000>=b1*1000):#zhang\n",
    "            b=1\n",
    "        else:\n",
    "            b=0\n",
    "        dataY.append(b)\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226872, 3, 15)\n"
     ]
    }
   ],
   "source": [
    "look_back = 3\n",
    "look_lag = 1 #用前20个数据预测当前1个数据\n",
    "train_size = 226875\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size-look_back:len(dataset),:]\n",
    "trainX, trainY = create_dataset(train, look_back,look_lag,1)\n",
    "testX, testY = create_dataset(test, look_back,look_lag,1)\n",
    "print(trainX.shape)\n",
    "features_set = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1]*15))\n",
    "scaler = StandardScaler()\n",
    "features_set_scaler = scaler.fit_transform(features_set)\n",
    "test_features = numpy.reshape(testX, (testX.shape[0], testX.shape[1]*15))\n",
    "test_features_scaler = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(testY,testPredict):\n",
    "    from sklearn.metrics import accuracy_score,classification_report\n",
    "    acc = accuracy_score(testY,testPredict)\n",
    "    report = classification_report(testY,testPredict)\n",
    "    print(\"acc:\",acc)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5286018772257305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.35      0.43     37571\n",
      "           1       0.52      0.70      0.60     37966\n",
      "\n",
      "    accuracy                           0.53     75537\n",
      "   macro avg       0.53      0.53      0.51     75537\n",
      "weighted avg       0.53      0.53      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model1 = LR(max_iter=1000) \n",
    "'''\n",
    "\n",
    "'''\n",
    "model1.fit(features_set, trainY)\n",
    "testPredict1 = model1.predict(test_features)  \n",
    "result(testY,testPredict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5385307862372083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.42      0.47     37571\n",
      "           1       0.53      0.66      0.59     37966\n",
      "\n",
      "    accuracy                           0.54     75537\n",
      "   macro avg       0.54      0.54      0.53     75537\n",
      "weighted avg       0.54      0.54      0.53     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model1 = LR(max_iter=1000) \n",
    "'''\n",
    "\n",
    "'''\n",
    "model1.fit(features_set_scaler, trainY)\n",
    "testPredict1 = model1.predict(test_features_scaler)  \n",
    "result(testY,testPredict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5375246567907118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.46      0.49     37571\n",
      "           1       0.53      0.62      0.57     37966\n",
      "\n",
      "    accuracy                           0.54     75537\n",
      "   macro avg       0.54      0.54      0.53     75537\n",
      "weighted avg       0.54      0.54      0.53     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model2 = LinearDiscriminantAnalysis() \n",
    "'''\n",
    "solver='lsqr', shrinkage=None, priors=None, n_components=None, store_covariance=False\n",
    "'''\n",
    "model2.fit(features_set, trainY)\n",
    "testPredict2 = model2.predict(test_features)  \n",
    "result(testY,testPredict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.517018150045673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.47      0.49     37571\n",
      "           1       0.52      0.56      0.54     37966\n",
      "\n",
      "    accuracy                           0.52     75537\n",
      "   macro avg       0.52      0.52      0.52     75537\n",
      "weighted avg       0.52      0.52      0.52     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model2 = LinearDiscriminantAnalysis() \n",
    "'''\n",
    "solver='lsqr', shrinkage=None, priors=None, n_components=None, store_covariance=False\n",
    "'''\n",
    "model2.fit(features_set_scaler, trainY)\n",
    "testPredict2 = model2.predict(test_features_scaler)  \n",
    "result(testY,testPredict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5084528112051048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.65      0.57     37571\n",
      "           1       0.52      0.37      0.43     37966\n",
      "\n",
      "    accuracy                           0.51     75537\n",
      "   macro avg       0.51      0.51      0.50     75537\n",
      "weighted avg       0.51      0.51      0.50     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model3 = QuadraticDiscriminantAnalysis() \n",
    "model3.fit(features_set, trainY)\n",
    "testPredict3 = model3.predict(test_features)  \n",
    "result(testY,testPredict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5027072825237963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.00      0.00     37571\n",
      "           1       0.50      1.00      0.67     37966\n",
      "\n",
      "    accuracy                           0.50     75537\n",
      "   macro avg       0.51      0.50      0.34     75537\n",
      "weighted avg       0.51      0.50      0.34     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model3 = QuadraticDiscriminantAnalysis() \n",
    "model3.fit(features_set_scaler, trainY)\n",
    "testPredict3 = model3.predict(test_features_scaler)  \n",
    "result(testY,testPredict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting xgboost\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/7c/32/a11befbb003e0e6b7e062a77f010dfcec0ec3589be537b02d2eb2ff93b9a/xgboost-1.1.1-py3-none-manylinux2010_x86_64.whl (127.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 127.6 MB 3.1 MB/s eta 0:00:01��██████████████▋             | 74.4 MB 2.8 MB/s eta 0:00:19     |█████████████████████████████   | 115.8 MB 3.1 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from xgboost) (1.18.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5258879754292598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.49      0.51     37571\n",
      "           1       0.53      0.56      0.54     37966\n",
      "\n",
      "    accuracy                           0.53     75537\n",
      "   macro avg       0.53      0.53      0.53     75537\n",
      "weighted avg       0.53      0.53      0.53     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model4 = xgb.XGBClassifier()\n",
    "'''\n",
    "objective ='reg:logistic', colsample_bytree = 0.5, learning_rate = 0.003,\n",
    "                max_depth = 20, alpha = 20, n_estimators = 400\n",
    "'''\n",
    "model4.fit(features_set, trainY)\n",
    "testPredict4 = model4.predict(test_features)  \n",
    "result(testY,testPredict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.518368481671234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.40      0.45     37571\n",
      "           1       0.52      0.64      0.57     37966\n",
      "\n",
      "    accuracy                           0.52     75537\n",
      "   macro avg       0.52      0.52      0.51     75537\n",
      "weighted avg       0.52      0.52      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model4 = xgb.XGBClassifier()\n",
    "'''\n",
    "objective ='reg:logistic', colsample_bytree = 0.5, learning_rate = 0.003,\n",
    "                max_depth = 20, alpha = 20, n_estimators = 400\n",
    "'''\n",
    "model4.fit(features_set_scaler, trainY)\n",
    "testPredict4 = model4.predict(test_features_scaler)  \n",
    "result(testY,testPredict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5026146127063559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     37571\n",
      "           1       0.50      1.00      0.67     37966\n",
      "\n",
      "    accuracy                           0.50     75537\n",
      "   macro avg       0.25      0.50      0.33     75537\n",
      "weighted avg       0.25      0.50      0.34     75537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model5 = svm.SVC()\n",
    "model5.fit(features_set, trainY)\n",
    "testPredict5 = model5.predict(test_features)  \n",
    "result(testY,testPredict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5210823834677045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.39      0.44     37571\n",
      "           1       0.52      0.66      0.58     37966\n",
      "\n",
      "    accuracy                           0.52     75537\n",
      "   macro avg       0.52      0.52      0.51     75537\n",
      "weighted avg       0.52      0.52      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model5 = svm.SVC()\n",
    "model5.fit(features_set_scaler, trainY)\n",
    "testPredict5 = model5.predict(test_features_scaler)  \n",
    "result(testY,testPredict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5093662708341608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.51     37571\n",
      "           1       0.51      0.50      0.50     37966\n",
      "\n",
      "    accuracy                           0.51     75537\n",
      "   macro avg       0.51      0.51      0.51     75537\n",
      "weighted avg       0.51      0.51      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier()\n",
    "regr.fit(features_set, trainY)\n",
    "testPredict6 = regr.predict(test_features)  \n",
    "result(testY,testPredict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5107695566411162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.48      0.50     37571\n",
      "           1       0.51      0.54      0.52     37966\n",
      "\n",
      "    accuracy                           0.51     75537\n",
      "   macro avg       0.51      0.51      0.51     75537\n",
      "weighted avg       0.51      0.51      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier()\n",
    "regr.fit(features_set_scaler, trainY)\n",
    "testPredict6 = regr.predict(test_features_scaler)  \n",
    "result(testY,testPredict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "dataset_scaler = scaler1.fit_transform(dataset)\n",
    "look_back = 3\n",
    "look_lag = 1 #用前20个数据预测当前1个数据\n",
    "train_size = 226875\n",
    "test_size = len(dataset_scaler) - train_size\n",
    "train, test = dataset_scaler[0:train_size,:], dataset_scaler[train_size-look_back:len(dataset),:]\n",
    "trainX, trainY = create_dataset(train, look_back,look_lag,1)\n",
    "testX, testY = create_dataset(test, look_back,look_lag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08440489, 0.08450881, 0.0858232 , ..., 0.47406096,\n",
       "         0.04008179, 0.61731654],\n",
       "        [0.08433812, 0.08454686, 0.08596596, ..., 0.49966189,\n",
       "         0.03967699, 0.60533786],\n",
       "        [0.08433344, 0.08454897, 0.08597486, ..., 0.52343418,\n",
       "         0.03904597, 0.59288706]],\n",
       "\n",
       "       [[0.08433812, 0.08454686, 0.08596596, ..., 0.49966189,\n",
       "         0.03967699, 0.60533786],\n",
       "        [0.08433344, 0.08454897, 0.08597486, ..., 0.52343418,\n",
       "         0.03904597, 0.59288706],\n",
       "        [0.08432256, 0.08453976, 0.08596733, ..., 0.5455943 ,\n",
       "         0.03843396, 0.59053584]],\n",
       "\n",
       "       [[0.08433344, 0.08454897, 0.08597486, ..., 0.52343418,\n",
       "         0.03904597, 0.59288706],\n",
       "        [0.08432256, 0.08453976, 0.08596733, ..., 0.5455943 ,\n",
       "         0.03843396, 0.59053584],\n",
       "        [0.08427648, 0.0845135 , 0.08596095, ..., 0.55834151,\n",
       "         0.0380517 , 0.57602388]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.2992561 , 0.29959318, 0.30064681, ..., 0.17401317,\n",
       "         0.02045163, 0.65789862],\n",
       "        [0.29947628, 0.29968484, 0.30060977, ..., 0.19535328,\n",
       "         0.02102246, 0.78627163],\n",
       "        [0.2996677 , 0.299809  , 0.30066639, ..., 0.22313379,\n",
       "         0.02163086, 0.79427795]],\n",
       "\n",
       "       [[0.29947628, 0.29968484, 0.30060977, ..., 0.19535328,\n",
       "         0.02102246, 0.78627163],\n",
       "        [0.2996677 , 0.299809  , 0.30066639, ..., 0.22313379,\n",
       "         0.02163086, 0.79427795],\n",
       "        [0.29986447, 0.29994352, 0.30073836, ..., 0.25339422,\n",
       "         0.02214458, 0.75275294]],\n",
       "\n",
       "       [[0.2996677 , 0.299809  , 0.30066639, ..., 0.22313379,\n",
       "         0.02163086, 0.79427795],\n",
       "        [0.29986447, 0.29994352, 0.30073836, ..., 0.25339422,\n",
       "         0.02214458, 0.75275294],\n",
       "        [0.30003799, 0.30009413, 0.30086571, ..., 0.2854934 ,\n",
       "         0.0226743 , 0.71971715]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras==2.2.4\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 860 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.18.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.5.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 152004 samples, validate on 74868 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "152004/152004 [==============================] - 39s 257us/step - loss: 0.6927 - acc: 0.5154 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 2/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6926 - acc: 0.5158 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 3/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 4/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 5/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 6/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 7/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 8/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 9/200\n",
      "152004/152004 [==============================] - 30s 197us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6937 - val_acc: 0.5025\n",
      "Epoch 10/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6925 - acc: 0.5158 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 11/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6924 - acc: 0.5158 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 12/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6923 - acc: 0.5156 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 13/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6923 - acc: 0.5154 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 14/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6932 - val_acc: 0.5024\n",
      "Epoch 15/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6922 - acc: 0.5157 - val_loss: 0.6931 - val_acc: 0.5025\n",
      "Epoch 16/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5159 - val_loss: 0.6943 - val_acc: 0.5025\n",
      "Epoch 17/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6922 - acc: 0.5157 - val_loss: 0.6933 - val_acc: 0.5024\n",
      "Epoch 18/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 19/200\n",
      "152004/152004 [==============================] - 30s 197us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 20/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6934 - val_acc: 0.5024\n",
      "Epoch 21/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5154 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 22/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 23/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 24/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 25/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6921 - acc: 0.5161 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 26/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 27/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5165 - val_loss: 0.6932 - val_acc: 0.5024\n",
      "Epoch 28/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.5023\n",
      "Epoch 29/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6920 - acc: 0.5159 - val_loss: 0.6933 - val_acc: 0.5024\n",
      "Epoch 30/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6942 - val_acc: 0.5023\n",
      "Epoch 31/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 32/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5156 - val_loss: 0.6938 - val_acc: 0.5022\n",
      "Epoch 33/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5156 - val_loss: 0.6941 - val_acc: 0.5025\n",
      "Epoch 34/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5159 - val_loss: 0.6941 - val_acc: 0.5025\n",
      "Epoch 35/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5158 - val_loss: 0.6940 - val_acc: 0.5025\n",
      "Epoch 36/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5157 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 37/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6920 - acc: 0.5159 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 38/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6920 - acc: 0.5161 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 39/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5160 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 40/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6920 - acc: 0.5159 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 41/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5158 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 42/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5163 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 43/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5158 - val_loss: 0.6931 - val_acc: 0.5029\n",
      "Epoch 44/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6919 - acc: 0.5157 - val_loss: 0.6950 - val_acc: 0.5025\n",
      "Epoch 45/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5158 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 46/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5159 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 47/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6918 - acc: 0.5161 - val_loss: 0.6931 - val_acc: 0.5023\n",
      "Epoch 48/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6919 - acc: 0.5158 - val_loss: 0.6932 - val_acc: 0.5021\n",
      "Epoch 49/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6918 - acc: 0.5157 - val_loss: 0.6933 - val_acc: 0.5024\n",
      "Epoch 50/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6918 - acc: 0.5161 - val_loss: 0.6950 - val_acc: 0.5025\n",
      "Epoch 51/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6917 - acc: 0.5160 - val_loss: 0.6942 - val_acc: 0.5025\n",
      "Epoch 52/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6917 - acc: 0.5163 - val_loss: 0.6931 - val_acc: 0.5028\n",
      "Epoch 53/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6917 - acc: 0.5156 - val_loss: 0.6955 - val_acc: 0.5027\n",
      "Epoch 54/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6916 - acc: 0.5158 - val_loss: 0.6933 - val_acc: 0.5027\n",
      "Epoch 55/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6915 - acc: 0.5173 - val_loss: 0.6944 - val_acc: 0.5024\n",
      "Epoch 56/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6916 - acc: 0.5159 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 57/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6916 - acc: 0.5167 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 58/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6916 - acc: 0.5161 - val_loss: 0.6940 - val_acc: 0.5022\n",
      "Epoch 59/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6915 - acc: 0.5167 - val_loss: 0.6939 - val_acc: 0.5019\n",
      "Epoch 60/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6915 - acc: 0.5155 - val_loss: 0.6934 - val_acc: 0.5027\n",
      "Epoch 61/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6915 - acc: 0.5154 - val_loss: 0.6952 - val_acc: 0.5031\n",
      "Epoch 62/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6914 - acc: 0.5168 - val_loss: 0.6934 - val_acc: 0.5027\n",
      "Epoch 63/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6914 - acc: 0.5171 - val_loss: 0.6940 - val_acc: 0.5027\n",
      "Epoch 64/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6915 - acc: 0.5166 - val_loss: 0.6933 - val_acc: 0.5021\n",
      "Epoch 65/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6912 - acc: 0.5178 - val_loss: 0.6961 - val_acc: 0.5031\n",
      "Epoch 66/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6911 - acc: 0.5172 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 67/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6912 - acc: 0.5179 - val_loss: 0.6948 - val_acc: 0.5032\n",
      "Epoch 68/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6912 - acc: 0.5156 - val_loss: 0.6944 - val_acc: 0.5037\n",
      "Epoch 69/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6912 - acc: 0.5165 - val_loss: 0.6933 - val_acc: 0.5032\n",
      "Epoch 70/200\n",
      "152004/152004 [==============================] - 30s 197us/step - loss: 0.6912 - acc: 0.5164 - val_loss: 0.6934 - val_acc: 0.5046\n",
      "Epoch 71/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6912 - acc: 0.5160 - val_loss: 0.6930 - val_acc: 0.5059\n",
      "Epoch 72/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6912 - acc: 0.5173 - val_loss: 0.6955 - val_acc: 0.5049\n",
      "Epoch 73/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6912 - acc: 0.5168 - val_loss: 0.6931 - val_acc: 0.5045\n",
      "Epoch 74/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6911 - acc: 0.5183 - val_loss: 0.6944 - val_acc: 0.5032\n",
      "Epoch 75/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6911 - acc: 0.5169 - val_loss: 0.6938 - val_acc: 0.5029\n",
      "Epoch 76/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6911 - acc: 0.5169 - val_loss: 0.6945 - val_acc: 0.5054\n",
      "Epoch 77/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6910 - acc: 0.5177 - val_loss: 0.6951 - val_acc: 0.5041\n",
      "Epoch 78/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6910 - acc: 0.5183 - val_loss: 0.6947 - val_acc: 0.5035\n",
      "Epoch 79/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6911 - acc: 0.5182 - val_loss: 0.6950 - val_acc: 0.5046\n",
      "Epoch 80/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6909 - acc: 0.5191 - val_loss: 0.6967 - val_acc: 0.5080\n",
      "Epoch 81/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6910 - acc: 0.5195 - val_loss: 0.6936 - val_acc: 0.5045\n",
      "Epoch 82/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6910 - acc: 0.5197 - val_loss: 0.6935 - val_acc: 0.5096\n",
      "Epoch 83/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6909 - acc: 0.5189 - val_loss: 0.6942 - val_acc: 0.5063\n",
      "Epoch 84/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6909 - acc: 0.5197 - val_loss: 0.6942 - val_acc: 0.5033\n",
      "Epoch 85/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6909 - acc: 0.5208 - val_loss: 0.6941 - val_acc: 0.5053\n",
      "Epoch 86/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6909 - acc: 0.5200 - val_loss: 0.6932 - val_acc: 0.5077\n",
      "Epoch 87/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6908 - acc: 0.5205 - val_loss: 0.6938 - val_acc: 0.5100\n",
      "Epoch 88/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6909 - acc: 0.5216 - val_loss: 0.6944 - val_acc: 0.5083\n",
      "Epoch 89/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6905 - acc: 0.5224 - val_loss: 0.6941 - val_acc: 0.5108\n",
      "Epoch 90/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6906 - acc: 0.5233 - val_loss: 0.6936 - val_acc: 0.5041\n",
      "Epoch 91/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6904 - acc: 0.5242 - val_loss: 0.6930 - val_acc: 0.5143\n",
      "Epoch 92/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6904 - acc: 0.5249 - val_loss: 0.6934 - val_acc: 0.5108\n",
      "Epoch 93/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6903 - acc: 0.5270 - val_loss: 0.6931 - val_acc: 0.5113\n",
      "Epoch 94/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6902 - acc: 0.5270 - val_loss: 0.6938 - val_acc: 0.5120\n",
      "Epoch 95/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6902 - acc: 0.5259 - val_loss: 0.6934 - val_acc: 0.5077\n",
      "Epoch 96/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6901 - acc: 0.5272 - val_loss: 0.6928 - val_acc: 0.5150\n",
      "Epoch 97/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6900 - acc: 0.5289 - val_loss: 0.6933 - val_acc: 0.5143\n",
      "Epoch 98/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6900 - acc: 0.5278 - val_loss: 0.6940 - val_acc: 0.5117\n",
      "Epoch 99/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6900 - acc: 0.5278 - val_loss: 0.6937 - val_acc: 0.5119\n",
      "Epoch 100/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6899 - acc: 0.5287 - val_loss: 0.6954 - val_acc: 0.5146\n",
      "Epoch 101/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6899 - acc: 0.5297 - val_loss: 0.6925 - val_acc: 0.5170\n",
      "Epoch 102/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6898 - acc: 0.5305 - val_loss: 0.6942 - val_acc: 0.5130\n",
      "Epoch 103/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6898 - acc: 0.5298 - val_loss: 0.6934 - val_acc: 0.5173\n",
      "Epoch 104/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6899 - acc: 0.5296 - val_loss: 0.6933 - val_acc: 0.5159\n",
      "Epoch 105/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6898 - acc: 0.5298 - val_loss: 0.6938 - val_acc: 0.5133\n",
      "Epoch 106/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6898 - acc: 0.5304 - val_loss: 0.6931 - val_acc: 0.5128\n",
      "Epoch 107/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6898 - acc: 0.5282 - val_loss: 0.6940 - val_acc: 0.5137\n",
      "Epoch 108/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6898 - acc: 0.5305 - val_loss: 0.6929 - val_acc: 0.5179\n",
      "Epoch 109/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6896 - acc: 0.5309 - val_loss: 0.6934 - val_acc: 0.5149\n",
      "Epoch 110/200\n",
      "152004/152004 [==============================] - 30s 198us/step - loss: 0.6895 - acc: 0.5311 - val_loss: 0.6930 - val_acc: 0.5201\n",
      "Epoch 111/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6895 - acc: 0.5298 - val_loss: 0.6926 - val_acc: 0.5195\n",
      "Epoch 112/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6895 - acc: 0.5311 - val_loss: 0.6942 - val_acc: 0.5102\n",
      "Epoch 113/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6896 - acc: 0.5305 - val_loss: 0.6933 - val_acc: 0.5147\n",
      "Epoch 114/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6895 - acc: 0.5300 - val_loss: 0.6945 - val_acc: 0.5108\n",
      "Epoch 115/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6894 - acc: 0.5315 - val_loss: 0.6931 - val_acc: 0.5119\n",
      "Epoch 116/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6896 - acc: 0.5310 - val_loss: 0.6944 - val_acc: 0.5127\n",
      "Epoch 117/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6896 - acc: 0.5302 - val_loss: 0.6927 - val_acc: 0.5198\n",
      "Epoch 118/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6894 - acc: 0.5318 - val_loss: 0.6942 - val_acc: 0.5137\n",
      "Epoch 119/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6893 - acc: 0.5311 - val_loss: 0.6936 - val_acc: 0.5204\n",
      "Epoch 120/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6895 - acc: 0.5308 - val_loss: 0.6929 - val_acc: 0.5183\n",
      "Epoch 121/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6893 - acc: 0.5322 - val_loss: 0.6936 - val_acc: 0.5185\n",
      "Epoch 122/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6894 - acc: 0.5317 - val_loss: 0.6947 - val_acc: 0.5161\n",
      "Epoch 123/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6894 - acc: 0.5315 - val_loss: 0.6935 - val_acc: 0.5148\n",
      "Epoch 124/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6892 - acc: 0.5310 - val_loss: 0.6939 - val_acc: 0.5135\n",
      "Epoch 125/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6893 - acc: 0.5325 - val_loss: 0.6933 - val_acc: 0.5168\n",
      "Epoch 126/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6892 - acc: 0.5314 - val_loss: 0.6939 - val_acc: 0.5129\n",
      "Epoch 127/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6893 - acc: 0.5325 - val_loss: 0.6939 - val_acc: 0.5166\n",
      "Epoch 128/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6891 - acc: 0.5335 - val_loss: 0.6927 - val_acc: 0.5236\n",
      "Epoch 129/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6893 - acc: 0.5312 - val_loss: 0.6924 - val_acc: 0.5210\n",
      "Epoch 130/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6894 - acc: 0.5315 - val_loss: 0.6927 - val_acc: 0.5221\n",
      "Epoch 131/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6892 - acc: 0.5324 - val_loss: 0.6933 - val_acc: 0.5203\n",
      "Epoch 132/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6892 - acc: 0.5320 - val_loss: 0.6928 - val_acc: 0.5217\n",
      "Epoch 133/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6892 - acc: 0.5319 - val_loss: 0.6951 - val_acc: 0.5141\n",
      "Epoch 134/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6891 - acc: 0.5324 - val_loss: 0.6921 - val_acc: 0.5207\n",
      "Epoch 135/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5338 - val_loss: 0.6927 - val_acc: 0.5182\n",
      "Epoch 136/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5332 - val_loss: 0.6927 - val_acc: 0.5205\n",
      "Epoch 137/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6891 - acc: 0.5334 - val_loss: 0.6929 - val_acc: 0.5198\n",
      "Epoch 138/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5333 - val_loss: 0.6923 - val_acc: 0.5231\n",
      "Epoch 139/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5321 - val_loss: 0.6932 - val_acc: 0.5187\n",
      "Epoch 140/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5344 - val_loss: 0.6934 - val_acc: 0.5166\n",
      "Epoch 141/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6891 - acc: 0.5327 - val_loss: 0.6930 - val_acc: 0.5167\n",
      "Epoch 142/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6891 - acc: 0.5337 - val_loss: 0.6927 - val_acc: 0.5176\n",
      "Epoch 143/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5327 - val_loss: 0.6942 - val_acc: 0.5178\n",
      "Epoch 144/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6891 - acc: 0.5322 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 145/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6891 - acc: 0.5337 - val_loss: 0.6934 - val_acc: 0.5195\n",
      "Epoch 146/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5332 - val_loss: 0.6929 - val_acc: 0.5208\n",
      "Epoch 147/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6889 - acc: 0.5327 - val_loss: 0.6925 - val_acc: 0.5184\n",
      "Epoch 148/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6891 - acc: 0.5326 - val_loss: 0.6928 - val_acc: 0.5207\n",
      "Epoch 149/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5337 - val_loss: 0.6928 - val_acc: 0.5195\n",
      "Epoch 150/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5339 - val_loss: 0.6932 - val_acc: 0.5198\n",
      "Epoch 151/200\n",
      "152004/152004 [==============================] - 30s 197us/step - loss: 0.6889 - acc: 0.5330 - val_loss: 0.6924 - val_acc: 0.5207\n",
      "Epoch 152/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6889 - acc: 0.5339 - val_loss: 0.6940 - val_acc: 0.5127\n",
      "Epoch 153/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5343 - val_loss: 0.6932 - val_acc: 0.5198\n",
      "Epoch 154/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6889 - acc: 0.5336 - val_loss: 0.6930 - val_acc: 0.5184\n",
      "Epoch 155/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5331 - val_loss: 0.6927 - val_acc: 0.5216\n",
      "Epoch 156/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5324 - val_loss: 0.6925 - val_acc: 0.5209\n",
      "Epoch 157/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6889 - acc: 0.5333 - val_loss: 0.6930 - val_acc: 0.5200\n",
      "Epoch 158/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6889 - acc: 0.5343 - val_loss: 0.6927 - val_acc: 0.5202\n",
      "Epoch 159/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6890 - acc: 0.5321 - val_loss: 0.6944 - val_acc: 0.5139\n",
      "Epoch 160/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6888 - acc: 0.5344 - val_loss: 0.6925 - val_acc: 0.5188\n",
      "Epoch 161/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6889 - acc: 0.5338 - val_loss: 0.6938 - val_acc: 0.5186\n",
      "Epoch 162/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5337 - val_loss: 0.6931 - val_acc: 0.5233\n",
      "Epoch 163/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5340 - val_loss: 0.6940 - val_acc: 0.5182\n",
      "Epoch 164/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5349 - val_loss: 0.6925 - val_acc: 0.5198\n",
      "Epoch 165/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6887 - acc: 0.5338 - val_loss: 0.6930 - val_acc: 0.5201\n",
      "Epoch 166/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5354 - val_loss: 0.6934 - val_acc: 0.5202\n",
      "Epoch 167/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6886 - acc: 0.5350 - val_loss: 0.6927 - val_acc: 0.5209\n",
      "Epoch 168/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5355 - val_loss: 0.6939 - val_acc: 0.5188\n",
      "Epoch 169/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6887 - acc: 0.5349 - val_loss: 0.6941 - val_acc: 0.5149\n",
      "Epoch 170/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5335 - val_loss: 0.6923 - val_acc: 0.5227\n",
      "Epoch 171/200\n",
      "152004/152004 [==============================] - 30s 197us/step - loss: 0.6888 - acc: 0.5336 - val_loss: 0.6925 - val_acc: 0.5191\n",
      "Epoch 172/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5341 - val_loss: 0.6932 - val_acc: 0.5185\n",
      "Epoch 173/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5344 - val_loss: 0.6927 - val_acc: 0.5209\n",
      "Epoch 174/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5344 - val_loss: 0.6940 - val_acc: 0.5199\n",
      "Epoch 175/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6887 - acc: 0.5356 - val_loss: 0.6931 - val_acc: 0.5209\n",
      "Epoch 176/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5345 - val_loss: 0.6933 - val_acc: 0.5209\n",
      "Epoch 177/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6888 - acc: 0.5324 - val_loss: 0.6927 - val_acc: 0.5210\n",
      "Epoch 178/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5336 - val_loss: 0.6941 - val_acc: 0.5157\n",
      "Epoch 179/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5358 - val_loss: 0.6928 - val_acc: 0.5243\n",
      "Epoch 180/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6888 - acc: 0.5336 - val_loss: 0.6932 - val_acc: 0.5189\n",
      "Epoch 181/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6887 - acc: 0.5340 - val_loss: 0.6932 - val_acc: 0.5194\n",
      "Epoch 182/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6886 - acc: 0.5347 - val_loss: 0.6926 - val_acc: 0.5221\n",
      "Epoch 183/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6886 - acc: 0.5345 - val_loss: 0.6930 - val_acc: 0.5191\n",
      "Epoch 184/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5354 - val_loss: 0.6935 - val_acc: 0.5165\n",
      "Epoch 185/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5340 - val_loss: 0.6931 - val_acc: 0.5171\n",
      "Epoch 186/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5344 - val_loss: 0.6927 - val_acc: 0.5239\n",
      "Epoch 187/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6887 - acc: 0.5352 - val_loss: 0.6929 - val_acc: 0.5219\n",
      "Epoch 188/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6885 - acc: 0.5350 - val_loss: 0.6931 - val_acc: 0.5185\n",
      "Epoch 189/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5357 - val_loss: 0.6923 - val_acc: 0.5238\n",
      "Epoch 190/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6885 - acc: 0.5333 - val_loss: 0.6942 - val_acc: 0.5172\n",
      "Epoch 191/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6887 - acc: 0.5328 - val_loss: 0.6933 - val_acc: 0.5206\n",
      "Epoch 192/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6886 - acc: 0.5355 - val_loss: 0.6926 - val_acc: 0.5229\n",
      "Epoch 193/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6885 - acc: 0.5354 - val_loss: 0.6949 - val_acc: 0.5203\n",
      "Epoch 194/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6886 - acc: 0.5352 - val_loss: 0.6936 - val_acc: 0.5213\n",
      "Epoch 195/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6885 - acc: 0.5349 - val_loss: 0.6924 - val_acc: 0.5228\n",
      "Epoch 196/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6885 - acc: 0.5334 - val_loss: 0.6935 - val_acc: 0.5186\n",
      "Epoch 197/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6884 - acc: 0.5346 - val_loss: 0.6927 - val_acc: 0.5191\n",
      "Epoch 198/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6886 - acc: 0.5356 - val_loss: 0.6937 - val_acc: 0.5205\n",
      "Epoch 199/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6885 - acc: 0.5357 - val_loss: 0.6928 - val_acc: 0.5214\n",
      "Epoch 200/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6885 - acc: 0.5360 - val_loss: 0.6924 - val_acc: 0.5232\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Conv1D,Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "%matplotlib inline\n",
    "from keras import optimizers,initializers,regularizers\n",
    "model = Sequential() \n",
    "\n",
    "model.add(LSTM(units=16, return_sequences=True, \n",
    "               input_shape=(trainX.shape[1],trainX.shape[2])))#,kernel_regularizer=regularizers.l2(0.01)\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=16, return_sequences=True,activation='relu'))#,kernel_regularizer=regularizers.l2(0.01)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=16,return_sequences=False,activation='relu'))#,kernel_regularizer=regularizers.l2(0.01)\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(units = 1,activation='sigmoid'))#sigmoid\n",
    "model.compile(loss='binary_crossentropy',#binary_crossentropy\n",
    "                  optimizer='Adam',#RMSprop optimizers.Adam(lr=0.00001)\n",
    "                  metrics=['accuracy'])  #mean_squared_error metrics=['accuracy']\n",
    "history = model.fit(trainX,trainY,epochs=200,validation_split=0.33, batch_size=120, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5298463004885023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.56      0.54     37571\n",
      "           1       0.53      0.50      0.52     37966\n",
      "\n",
      "    accuracy                           0.53     75537\n",
      "   macro avg       0.53      0.53      0.53     75537\n",
      "weighted avg       0.53      0.53      0.53     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testPredict = model.predict_classes(testX)\n",
    "result(testY,testPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
