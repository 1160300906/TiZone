{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>BBANDS_middle</th>\n",
       "      <th>EMA</th>\n",
       "      <th>SMA</th>\n",
       "      <th>RSI</th>\n",
       "      <th>ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4269.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4269.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4252.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4251.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4246.92</td>\n",
       "      <td>4263.478797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     close  BBANDS_middle  EMA  SMA  RSI  ROC\n",
       "0  4269.26            NaN  NaN  NaN  NaN  NaN\n",
       "1  4269.01            NaN  NaN  NaN  NaN  NaN\n",
       "2  4252.01            NaN  NaN  NaN  NaN  NaN\n",
       "3  4251.67            NaN  NaN  NaN  NaN  NaN\n",
       "4  4246.92    4263.478797  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "dataframe1 = read_csv('newBinanceData1.csv', engine='python')\n",
    "dataframe = dataframe1[['close','BBANDS_middle','EMA','SMA','RSI','ROC']]\n",
    "'''\n",
    "'close','BBANDS_upper', 'BBANDS_middle', 'BBANDS_lower','EMA','MA','SMA','RSI','TRIX','ROC','macd'\n",
    "                                          , 'macdsignal', 'macdhist','ADX','ATR','CCI'\n",
    "'''\n",
    "dataset = dataframe.iloc[88:,:].values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float64')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.33000000e+03, 4.33603861e+03, 4.31550224e+03, 4.31067287e+03,\n",
       "        9.12871560e+01, 5.35136194e-01],\n",
       "       [4.33000000e+03, 4.33667486e+03, 4.31684256e+03, 4.31208530e+03,\n",
       "        8.88388216e+01, 3.98251137e-01],\n",
       "       [4.32900000e+03, 4.33671023e+03, 4.31805169e+03, 4.31347599e+03,\n",
       "        8.62266100e+01, 2.66439910e-01],\n",
       "       ...,\n",
       "       [9.28292000e+03, 9.28169976e+03, 9.28081507e+03, 9.28186162e+03,\n",
       "        6.02147811e+01, 2.30061840e-02],\n",
       "       [9.27901000e+03, 9.28203101e+03, 9.28090540e+03, 9.28191962e+03,\n",
       "        5.77832928e+01, 3.00643924e-02],\n",
       "       [9.27999000e+03, 9.28222596e+03, 9.28097623e+03, 9.28195118e+03,\n",
       "        5.62471782e+01, 3.06508466e-02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back,look_lag,temp):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(0,len(dataset)-look_back-look_lag+1,temp):\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        b1 = dataset[(i+look_back-1):(i+look_back),0]\n",
    "        b2 = dataset[(i+look_back):(i+look_back+look_lag),0]\n",
    "        if (b2*1000>=b1*1000):#zhang\n",
    "            b=1\n",
    "        else:\n",
    "            b=0\n",
    "        dataY.append(b)\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226872, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "look_back = 3\n",
    "look_lag = 1 #用前20个数据预测当前1个数据\n",
    "train_size = 226875\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size-look_back:len(dataset),:]\n",
    "trainX, trainY = create_dataset(train, look_back,look_lag,1)\n",
    "testX, testY = create_dataset(test, look_back,look_lag,1)\n",
    "print(trainX.shape)\n",
    "features_set = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1]*6))\n",
    "scaler = StandardScaler()\n",
    "features_set_scaler = scaler.fit_transform(features_set)\n",
    "test_features = numpy.reshape(testX, (testX.shape[0], testX.shape[1]*6))\n",
    "test_features_scaler = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(testY,testPredict):\n",
    "    from sklearn.metrics import accuracy_score,classification_report\n",
    "    acc = accuracy_score(testY,testPredict)\n",
    "    report = classification_report(testY,testPredict)\n",
    "    print(\"acc:\",acc)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5305611819373287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.23      0.33     37571\n",
      "           1       0.52      0.83      0.64     37966\n",
      "\n",
      "    accuracy                           0.53     75537\n",
      "   macro avg       0.55      0.53      0.48     75537\n",
      "weighted avg       0.55      0.53      0.48     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model1 = LR(max_iter=1000) \n",
    "'''\n",
    "\n",
    "'''\n",
    "model1.fit(features_set, trainY)\n",
    "testPredict1 = model1.predict(test_features)  \n",
    "result(testY,testPredict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5435084792882958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.41      0.47     37571\n",
      "           1       0.54      0.68      0.60     37966\n",
      "\n",
      "    accuracy                           0.54     75537\n",
      "   macro avg       0.55      0.54      0.53     75537\n",
      "weighted avg       0.55      0.54      0.53     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model1 = LR(max_iter=1000) \n",
    "'''\n",
    "\n",
    "'''\n",
    "model1.fit(features_set_scaler, trainY)\n",
    "testPredict1 = model1.predict(test_features_scaler)  \n",
    "result(testY,testPredict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5433628552894608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.42      0.48     37571\n",
      "           1       0.54      0.66      0.59     37966\n",
      "\n",
      "    accuracy                           0.54     75537\n",
      "   macro avg       0.55      0.54      0.54     75537\n",
      "weighted avg       0.55      0.54      0.54     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model2 = LinearDiscriminantAnalysis() \n",
    "'''\n",
    "solver='lsqr', shrinkage=None, priors=None, n_components=None, store_covariance=False\n",
    "'''\n",
    "model2.fit(features_set, trainY)\n",
    "testPredict2 = model2.predict(test_features)  \n",
    "result(testY,testPredict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5433760938348094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.41      0.47     37571\n",
      "           1       0.54      0.68      0.60     37966\n",
      "\n",
      "    accuracy                           0.54     75537\n",
      "   macro avg       0.55      0.54      0.53     75537\n",
      "weighted avg       0.55      0.54      0.53     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model2 = LinearDiscriminantAnalysis() \n",
    "'''\n",
    "solver='lsqr', shrinkage=None, priors=None, n_components=None, store_covariance=False\n",
    "'''\n",
    "model2.fit(features_set_scaler, trainY)\n",
    "testPredict2 = model2.predict(test_features_scaler)  \n",
    "result(testY,testPredict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5362008022558481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.57      0.55     37571\n",
      "           1       0.54      0.50      0.52     37966\n",
      "\n",
      "    accuracy                           0.54     75537\n",
      "   macro avg       0.54      0.54      0.54     75537\n",
      "weighted avg       0.54      0.54      0.54     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model3 = QuadraticDiscriminantAnalysis() \n",
    "model3.fit(features_set, trainY)\n",
    "testPredict3 = model3.predict(test_features)  \n",
    "result(testY,testPredict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4973986258389928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.66     37571\n",
      "           1       0.57      0.00      0.00     37966\n",
      "\n",
      "    accuracy                           0.50     75537\n",
      "   macro avg       0.53      0.50      0.33     75537\n",
      "weighted avg       0.53      0.50      0.33     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "model3 = QuadraticDiscriminantAnalysis() \n",
    "model3.fit(features_set_scaler, trainY)\n",
    "testPredict3 = model3.predict(test_features_scaler)  \n",
    "result(testY,testPredict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: xgboost in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (1.1.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from xgboost) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5263248474257649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.43      0.48     37571\n",
      "           1       0.52      0.62      0.57     37966\n",
      "\n",
      "    accuracy                           0.53     75537\n",
      "   macro avg       0.53      0.53      0.52     75537\n",
      "weighted avg       0.53      0.53      0.52     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model4 = xgb.XGBClassifier()\n",
    "'''\n",
    "objective ='reg:logistic', colsample_bytree = 0.5, learning_rate = 0.003,\n",
    "                max_depth = 20, alpha = 20, n_estimators = 400\n",
    "'''\n",
    "model4.fit(features_set, trainY)\n",
    "testPredict4 = model4.predict(test_features)  \n",
    "result(testY,testPredict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5206455114711995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.39      0.45     37571\n",
      "           1       0.52      0.65      0.58     37966\n",
      "\n",
      "    accuracy                           0.52     75537\n",
      "   macro avg       0.52      0.52      0.51     75537\n",
      "weighted avg       0.52      0.52      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model4 = xgb.XGBClassifier()\n",
    "'''\n",
    "objective ='reg:logistic', colsample_bytree = 0.5, learning_rate = 0.003,\n",
    "                max_depth = 20, alpha = 20, n_estimators = 400\n",
    "'''\n",
    "model4.fit(features_set_scaler, trainY)\n",
    "testPredict4 = model4.predict(test_features_scaler)  \n",
    "result(testY,testPredict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5026146127063559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     37571\n",
      "           1       0.50      1.00      0.67     37966\n",
      "\n",
      "    accuracy                           0.50     75537\n",
      "   macro avg       0.25      0.50      0.33     75537\n",
      "weighted avg       0.25      0.50      0.34     75537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model5 = svm.SVC()\n",
    "model5.fit(features_set, trainY)\n",
    "testPredict5 = model5.predict(test_features)  \n",
    "result(testY,testPredict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5187921151223903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.35      0.42     37571\n",
      "           1       0.52      0.69      0.59     37966\n",
      "\n",
      "    accuracy                           0.52     75537\n",
      "   macro avg       0.52      0.52      0.50     75537\n",
      "weighted avg       0.52      0.52      0.50     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model5 = svm.SVC()\n",
    "model5.fit(features_set_scaler, trainY)\n",
    "testPredict5 = model5.predict(test_features_scaler)  \n",
    "result(testY,testPredict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5098825741027576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.49      0.50     37571\n",
      "           1       0.51      0.53      0.52     37966\n",
      "\n",
      "    accuracy                           0.51     75537\n",
      "   macro avg       0.51      0.51      0.51     75537\n",
      "weighted avg       0.51      0.51      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier()\n",
    "regr.fit(features_set, trainY)\n",
    "testPredict6 = regr.predict(test_features)  \n",
    "result(testY,testPredict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5110210890027403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49     37571\n",
      "           1       0.51      0.55      0.53     37966\n",
      "\n",
      "    accuracy                           0.51     75537\n",
      "   macro avg       0.51      0.51      0.51     75537\n",
      "weighted avg       0.51      0.51      0.51     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier()\n",
    "regr.fit(features_set_scaler, trainY)\n",
    "testPredict6 = regr.predict(test_features_scaler)  \n",
    "result(testY,testPredict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "dataset_scaler = scaler1.fit_transform(dataset)\n",
    "look_back = 3\n",
    "look_lag = 1 #用前20个数据预测当前1个数据\n",
    "train_size = 226875\n",
    "test_size = len(dataset_scaler) - train_size\n",
    "train, test = dataset_scaler[0:train_size,:], dataset_scaler[train_size-look_back:len(dataset),:]\n",
    "trainX, trainY = create_dataset(train, look_back,look_lag,1)\n",
    "testX, testY = create_dataset(test, look_back,look_lag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08912761, 0.08450881, 0.08088179, 0.08076369, 0.91286832,\n",
       "         0.53414527],\n",
       "        [0.08912761, 0.08454686, 0.0809626 , 0.08084872, 0.88838254,\n",
       "         0.52731904],\n",
       "        [0.08906859, 0.08454897, 0.08103551, 0.08093245, 0.86225782,\n",
       "         0.52074583]],\n",
       "\n",
       "       [[0.08912761, 0.08454686, 0.0809626 , 0.08084872, 0.88838254,\n",
       "         0.52731904],\n",
       "        [0.08906859, 0.08454897, 0.08103551, 0.08093245, 0.86225782,\n",
       "         0.52074583],\n",
       "        [0.08990851, 0.08453976, 0.08110654, 0.08101544, 0.86669303,\n",
       "         0.52068934]],\n",
       "\n",
       "       [[0.08906859, 0.08454897, 0.08103551, 0.08093245, 0.86225782,\n",
       "         0.52074583],\n",
       "        [0.08990851, 0.08453976, 0.08110654, 0.08101544, 0.86669303,\n",
       "         0.52068934],\n",
       "        [0.08898005, 0.0845135 , 0.08116973, 0.08109558, 0.83338333,\n",
       "         0.51625845]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.30262071, 0.29959318, 0.29902354, 0.29888119, 0.59884878,\n",
       "         0.5086206 ],\n",
       "        [0.30423976, 0.29968484, 0.29904498, 0.29888476, 0.71335257,\n",
       "         0.51153665],\n",
       "        [0.30367725, 0.299809  , 0.29907466, 0.29889368, 0.76180409,\n",
       "         0.51351676]],\n",
       "\n",
       "       [[0.30423976, 0.29968484, 0.29904498, 0.29888476, 0.71335257,\n",
       "         0.51153665],\n",
       "        [0.30367725, 0.299809  , 0.29907466, 0.29889368, 0.76180409,\n",
       "         0.51351676],\n",
       "        [0.30390627, 0.29994352, 0.29911256, 0.29890831, 0.80015691,\n",
       "         0.51509831]],\n",
       "\n",
       "       [[0.30367725, 0.299809  , 0.29907466, 0.29889368, 0.76180409,\n",
       "         0.51351676],\n",
       "        [0.30390627, 0.29994352, 0.29911256, 0.29890831, 0.80015691,\n",
       "         0.51509831],\n",
       "        [0.30402078, 0.30009413, 0.2991579 , 0.29892798, 0.82907508,\n",
       "         0.51712293]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: keras==2.2.4 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 152004 samples, validate on 74868 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "152004/152004 [==============================] - 34s 221us/step - loss: 0.6927 - acc: 0.5156 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 2/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6926 - acc: 0.5157 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 3/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6926 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5025\n",
      "Epoch 4/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6925 - acc: 0.5157 - val_loss: 0.6931 - val_acc: 0.5025\n",
      "Epoch 5/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6925 - acc: 0.5151 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 6/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6924 - acc: 0.5157 - val_loss: 0.6934 - val_acc: 0.5024\n",
      "Epoch 7/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6923 - acc: 0.5157 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 8/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6922 - acc: 0.5156 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 9/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 10/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5156 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 11/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6932 - val_acc: 0.5023\n",
      "Epoch 12/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 13/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 14/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6922 - acc: 0.5159 - val_loss: 0.6937 - val_acc: 0.5025\n",
      "Epoch 15/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 16/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5154 - val_loss: 0.6937 - val_acc: 0.5025\n",
      "Epoch 17/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6934 - val_acc: 0.5024\n",
      "Epoch 18/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5155 - val_loss: 0.6932 - val_acc: 0.5023\n",
      "Epoch 19/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6922 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 20/200\n",
      "152004/152004 [==============================] - 31s 207us/step - loss: 0.6922 - acc: 0.5155 - val_loss: 0.6933 - val_acc: 0.5025\n",
      "Epoch 21/200\n",
      "152004/152004 [==============================] - 32s 212us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 22/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6922 - acc: 0.5158 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 23/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 24/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5160 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 25/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.5021\n",
      "Epoch 26/200\n",
      "152004/152004 [==============================] - 29s 191us/step - loss: 0.6921 - acc: 0.5156 - val_loss: 0.6932 - val_acc: 0.5022\n",
      "Epoch 27/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5163 - val_loss: 0.6933 - val_acc: 0.5013\n",
      "Epoch 28/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5156 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 29/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6938 - val_acc: 0.5023\n",
      "Epoch 30/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6922 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.5018\n",
      "Epoch 31/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 32/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5160 - val_loss: 0.6933 - val_acc: 0.5021\n",
      "Epoch 33/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5156 - val_loss: 0.6933 - val_acc: 0.5021\n",
      "Epoch 34/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 35/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6922 - acc: 0.5151 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 36/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6937 - val_acc: 0.5022\n",
      "Epoch 37/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5156 - val_loss: 0.6944 - val_acc: 0.5025\n",
      "Epoch 38/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6921 - acc: 0.5149 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 39/200\n",
      "152004/152004 [==============================] - 31s 202us/step - loss: 0.6921 - acc: 0.5164 - val_loss: 0.6936 - val_acc: 0.5021\n",
      "Epoch 40/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5161 - val_loss: 0.6936 - val_acc: 0.5024\n",
      "Epoch 41/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5156 - val_loss: 0.6941 - val_acc: 0.5025\n",
      "Epoch 42/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5156 - val_loss: 0.6933 - val_acc: 0.5021\n",
      "Epoch 43/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5164 - val_loss: 0.6936 - val_acc: 0.5024\n",
      "Epoch 44/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6941 - val_acc: 0.5021\n",
      "Epoch 45/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 46/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6941 - val_acc: 0.5025\n",
      "Epoch 47/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5154 - val_loss: 0.6940 - val_acc: 0.5025\n",
      "Epoch 48/200\n",
      "152004/152004 [==============================] - 30s 197us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6934 - val_acc: 0.5019\n",
      "Epoch 49/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6934 - val_acc: 0.5016\n",
      "Epoch 50/200\n",
      "152004/152004 [==============================] - 30s 198us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 51/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 52/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6940 - val_acc: 0.5025\n",
      "Epoch 53/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5152 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 54/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6938 - val_acc: 0.5029\n",
      "Epoch 55/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5162 - val_loss: 0.6938 - val_acc: 0.5024\n",
      "Epoch 56/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6937 - val_acc: 0.5025\n",
      "Epoch 57/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5167 - val_loss: 0.6942 - val_acc: 0.5025\n",
      "Epoch 58/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6935 - val_acc: 0.5026\n",
      "Epoch 59/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5155 - val_loss: 0.6938 - val_acc: 0.5024\n",
      "Epoch 60/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5153 - val_loss: 0.6935 - val_acc: 0.5020\n",
      "Epoch 61/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5161 - val_loss: 0.6937 - val_acc: 0.5057\n",
      "Epoch 62/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6936 - val_acc: 0.5023\n",
      "Epoch 63/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6935 - val_acc: 0.5041\n",
      "Epoch 64/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5167 - val_loss: 0.6937 - val_acc: 0.5025\n",
      "Epoch 65/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5157 - val_loss: 0.6934 - val_acc: 0.5036\n",
      "Epoch 66/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6920 - acc: 0.5158 - val_loss: 0.6934 - val_acc: 0.5010\n",
      "Epoch 67/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5161 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 68/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5158 - val_loss: 0.6940 - val_acc: 0.5026\n",
      "Epoch 69/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5159 - val_loss: 0.6937 - val_acc: 0.5024\n",
      "Epoch 70/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5161 - val_loss: 0.6935 - val_acc: 0.5025\n",
      "Epoch 71/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5160 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 72/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6921 - acc: 0.5160 - val_loss: 0.6935 - val_acc: 0.5023\n",
      "Epoch 73/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6921 - acc: 0.5154 - val_loss: 0.6939 - val_acc: 0.5024\n",
      "Epoch 74/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6921 - acc: 0.5151 - val_loss: 0.6938 - val_acc: 0.5025\n",
      "Epoch 75/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6921 - acc: 0.5165 - val_loss: 0.6936 - val_acc: 0.5017\n",
      "Epoch 76/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5167 - val_loss: 0.6935 - val_acc: 0.5031\n",
      "Epoch 77/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6920 - acc: 0.5162 - val_loss: 0.6939 - val_acc: 0.5027\n",
      "Epoch 78/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5155 - val_loss: 0.6939 - val_acc: 0.5026\n",
      "Epoch 79/200\n",
      "152004/152004 [==============================] - 31s 202us/step - loss: 0.6920 - acc: 0.5165 - val_loss: 0.6934 - val_acc: 0.5038\n",
      "Epoch 80/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6920 - acc: 0.5163 - val_loss: 0.6935 - val_acc: 0.5023\n",
      "Epoch 81/200\n",
      "152004/152004 [==============================] - 29s 191us/step - loss: 0.6920 - acc: 0.5157 - val_loss: 0.6935 - val_acc: 0.5020\n",
      "Epoch 82/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6920 - acc: 0.5162 - val_loss: 0.6934 - val_acc: 0.5023\n",
      "Epoch 83/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5161 - val_loss: 0.6939 - val_acc: 0.5017\n",
      "Epoch 84/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6919 - acc: 0.5159 - val_loss: 0.6934 - val_acc: 0.5022\n",
      "Epoch 85/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6920 - acc: 0.5170 - val_loss: 0.6942 - val_acc: 0.5025\n",
      "Epoch 86/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5164 - val_loss: 0.6934 - val_acc: 0.5025\n",
      "Epoch 87/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6919 - acc: 0.5156 - val_loss: 0.6940 - val_acc: 0.5025\n",
      "Epoch 88/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6919 - acc: 0.5165 - val_loss: 0.6941 - val_acc: 0.5022\n",
      "Epoch 89/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6919 - acc: 0.5156 - val_loss: 0.6936 - val_acc: 0.5025\n",
      "Epoch 90/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6920 - acc: 0.5158 - val_loss: 0.6935 - val_acc: 0.5022\n",
      "Epoch 91/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6919 - acc: 0.5159 - val_loss: 0.6944 - val_acc: 0.5025\n",
      "Epoch 92/200\n",
      "152004/152004 [==============================] - 31s 206us/step - loss: 0.6919 - acc: 0.5162 - val_loss: 0.6938 - val_acc: 0.5038\n",
      "Epoch 93/200\n",
      "152004/152004 [==============================] - 32s 211us/step - loss: 0.6918 - acc: 0.5152 - val_loss: 0.6936 - val_acc: 0.5033\n",
      "Epoch 94/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6918 - acc: 0.5184 - val_loss: 0.6935 - val_acc: 0.5042\n",
      "Epoch 95/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6918 - acc: 0.5167 - val_loss: 0.6939 - val_acc: 0.5037\n",
      "Epoch 96/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6917 - acc: 0.5179 - val_loss: 0.6936 - val_acc: 0.5031\n",
      "Epoch 97/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6917 - acc: 0.5169 - val_loss: 0.6938 - val_acc: 0.5026\n",
      "Epoch 98/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6917 - acc: 0.5169 - val_loss: 0.6935 - val_acc: 0.5043\n",
      "Epoch 99/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6916 - acc: 0.5174 - val_loss: 0.6935 - val_acc: 0.5038\n",
      "Epoch 100/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6916 - acc: 0.5178 - val_loss: 0.6931 - val_acc: 0.5055\n",
      "Epoch 101/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6916 - acc: 0.5168 - val_loss: 0.6944 - val_acc: 0.5024\n",
      "Epoch 102/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6915 - acc: 0.5176 - val_loss: 0.6937 - val_acc: 0.5034\n",
      "Epoch 103/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6914 - acc: 0.5185 - val_loss: 0.6937 - val_acc: 0.5036\n",
      "Epoch 104/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6914 - acc: 0.5182 - val_loss: 0.6933 - val_acc: 0.5040\n",
      "Epoch 105/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6914 - acc: 0.5195 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 106/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6913 - acc: 0.5196 - val_loss: 0.6934 - val_acc: 0.5033\n",
      "Epoch 107/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6913 - acc: 0.5200 - val_loss: 0.6933 - val_acc: 0.5050\n",
      "Epoch 108/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6913 - acc: 0.5197 - val_loss: 0.6932 - val_acc: 0.5063\n",
      "Epoch 109/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6912 - acc: 0.5207 - val_loss: 0.6941 - val_acc: 0.5048\n",
      "Epoch 110/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6911 - acc: 0.5203 - val_loss: 0.6930 - val_acc: 0.5083\n",
      "Epoch 111/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6911 - acc: 0.5224 - val_loss: 0.6933 - val_acc: 0.5064\n",
      "Epoch 112/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6911 - acc: 0.5220 - val_loss: 0.6938 - val_acc: 0.5051\n",
      "Epoch 113/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6910 - acc: 0.5212 - val_loss: 0.6935 - val_acc: 0.5086\n",
      "Epoch 114/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6909 - acc: 0.5228 - val_loss: 0.6931 - val_acc: 0.5101\n",
      "Epoch 115/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6910 - acc: 0.5211 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 116/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6909 - acc: 0.5222 - val_loss: 0.6931 - val_acc: 0.5048\n",
      "Epoch 117/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6907 - acc: 0.5235 - val_loss: 0.6941 - val_acc: 0.5114\n",
      "Epoch 118/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6906 - acc: 0.5249 - val_loss: 0.6926 - val_acc: 0.5134\n",
      "Epoch 119/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6906 - acc: 0.5245 - val_loss: 0.6933 - val_acc: 0.5102\n",
      "Epoch 120/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6905 - acc: 0.5245 - val_loss: 0.6940 - val_acc: 0.5084\n",
      "Epoch 121/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6906 - acc: 0.5250 - val_loss: 0.6931 - val_acc: 0.5120\n",
      "Epoch 122/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6903 - acc: 0.5262 - val_loss: 0.6934 - val_acc: 0.5073\n",
      "Epoch 123/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6904 - acc: 0.5264 - val_loss: 0.6929 - val_acc: 0.5115\n",
      "Epoch 124/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6905 - acc: 0.5239 - val_loss: 0.6929 - val_acc: 0.5099\n",
      "Epoch 125/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6903 - acc: 0.5259 - val_loss: 0.6929 - val_acc: 0.5125\n",
      "Epoch 126/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6902 - acc: 0.5257 - val_loss: 0.6937 - val_acc: 0.5135\n",
      "Epoch 127/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6902 - acc: 0.5272 - val_loss: 0.6934 - val_acc: 0.5125\n",
      "Epoch 128/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6902 - acc: 0.5274 - val_loss: 0.6942 - val_acc: 0.5062\n",
      "Epoch 129/200\n",
      "152004/152004 [==============================] - 30s 200us/step - loss: 0.6902 - acc: 0.5288 - val_loss: 0.6937 - val_acc: 0.5124\n",
      "Epoch 130/200\n",
      "152004/152004 [==============================] - 33s 217us/step - loss: 0.6902 - acc: 0.5273 - val_loss: 0.6931 - val_acc: 0.5132\n",
      "Epoch 131/200\n",
      "152004/152004 [==============================] - 32s 211us/step - loss: 0.6901 - acc: 0.5278 - val_loss: 0.6929 - val_acc: 0.5130\n",
      "Epoch 132/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6903 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5141\n",
      "Epoch 133/200\n",
      "152004/152004 [==============================] - 30s 196us/step - loss: 0.6901 - acc: 0.5285 - val_loss: 0.6926 - val_acc: 0.5178\n",
      "Epoch 134/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6901 - acc: 0.5281 - val_loss: 0.6925 - val_acc: 0.5159\n",
      "Epoch 135/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6903 - acc: 0.5280 - val_loss: 0.6931 - val_acc: 0.5133\n",
      "Epoch 136/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6900 - acc: 0.5289 - val_loss: 0.6928 - val_acc: 0.5144\n",
      "Epoch 137/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6899 - acc: 0.5280 - val_loss: 0.6932 - val_acc: 0.5090\n",
      "Epoch 138/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6899 - acc: 0.5283 - val_loss: 0.6930 - val_acc: 0.5178\n",
      "Epoch 139/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6901 - acc: 0.5267 - val_loss: 0.6930 - val_acc: 0.5148\n",
      "Epoch 140/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6900 - acc: 0.5281 - val_loss: 0.6927 - val_acc: 0.5148\n",
      "Epoch 141/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6899 - acc: 0.5292 - val_loss: 0.6939 - val_acc: 0.5099\n",
      "Epoch 142/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6898 - acc: 0.5297 - val_loss: 0.6935 - val_acc: 0.5143\n",
      "Epoch 143/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6900 - acc: 0.5278 - val_loss: 0.6931 - val_acc: 0.5169\n",
      "Epoch 144/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6900 - acc: 0.5278 - val_loss: 0.6930 - val_acc: 0.5182\n",
      "Epoch 145/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6899 - acc: 0.5294 - val_loss: 0.6935 - val_acc: 0.5149\n",
      "Epoch 146/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6899 - acc: 0.5301 - val_loss: 0.6928 - val_acc: 0.5152\n",
      "Epoch 147/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6898 - acc: 0.5305 - val_loss: 0.6931 - val_acc: 0.5160\n",
      "Epoch 148/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6898 - acc: 0.5302 - val_loss: 0.6927 - val_acc: 0.5150\n",
      "Epoch 149/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6899 - acc: 0.5306 - val_loss: 0.6928 - val_acc: 0.5169\n",
      "Epoch 150/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6899 - acc: 0.5284 - val_loss: 0.6924 - val_acc: 0.5196\n",
      "Epoch 151/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6898 - acc: 0.5286 - val_loss: 0.6929 - val_acc: 0.5158\n",
      "Epoch 152/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6898 - acc: 0.5279 - val_loss: 0.6925 - val_acc: 0.5149\n",
      "Epoch 153/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6897 - acc: 0.5289 - val_loss: 0.6929 - val_acc: 0.5182\n",
      "Epoch 154/200\n",
      "152004/152004 [==============================] - 31s 205us/step - loss: 0.6899 - acc: 0.5299 - val_loss: 0.6923 - val_acc: 0.5183\n",
      "Epoch 155/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6899 - acc: 0.5281 - val_loss: 0.6928 - val_acc: 0.5176\n",
      "Epoch 156/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6897 - acc: 0.5298 - val_loss: 0.6931 - val_acc: 0.5155\n",
      "Epoch 157/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6897 - acc: 0.5304 - val_loss: 0.6931 - val_acc: 0.5159\n",
      "Epoch 158/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5296 - val_loss: 0.6928 - val_acc: 0.5165\n",
      "Epoch 159/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6898 - acc: 0.5302 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 160/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6899 - acc: 0.5288 - val_loss: 0.6926 - val_acc: 0.5154\n",
      "Epoch 161/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5304 - val_loss: 0.6930 - val_acc: 0.5147\n",
      "Epoch 162/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6897 - acc: 0.5318 - val_loss: 0.6925 - val_acc: 0.5181\n",
      "Epoch 163/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6898 - acc: 0.5305 - val_loss: 0.6929 - val_acc: 0.5173\n",
      "Epoch 164/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6895 - acc: 0.5305 - val_loss: 0.6922 - val_acc: 0.5223\n",
      "Epoch 165/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6897 - acc: 0.5303 - val_loss: 0.6924 - val_acc: 0.5191\n",
      "Epoch 166/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5307 - val_loss: 0.6925 - val_acc: 0.5212\n",
      "Epoch 167/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6896 - acc: 0.5314 - val_loss: 0.6931 - val_acc: 0.5159\n",
      "Epoch 168/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6895 - acc: 0.5321 - val_loss: 0.6937 - val_acc: 0.5153\n",
      "Epoch 169/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6895 - acc: 0.5317 - val_loss: 0.6927 - val_acc: 0.5186\n",
      "Epoch 170/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6897 - acc: 0.5312 - val_loss: 0.6928 - val_acc: 0.5172\n",
      "Epoch 171/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6896 - acc: 0.5306 - val_loss: 0.6929 - val_acc: 0.5142\n",
      "Epoch 172/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6894 - acc: 0.5322 - val_loss: 0.6925 - val_acc: 0.5186\n",
      "Epoch 173/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6895 - acc: 0.5304 - val_loss: 0.6931 - val_acc: 0.5156\n",
      "Epoch 174/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6894 - acc: 0.5320 - val_loss: 0.6933 - val_acc: 0.5155\n",
      "Epoch 175/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5306 - val_loss: 0.6940 - val_acc: 0.5135\n",
      "Epoch 176/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6896 - acc: 0.5310 - val_loss: 0.6933 - val_acc: 0.5165\n",
      "Epoch 177/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5308 - val_loss: 0.6925 - val_acc: 0.5175\n",
      "Epoch 178/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6895 - acc: 0.5327 - val_loss: 0.6925 - val_acc: 0.5187\n",
      "Epoch 179/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6896 - acc: 0.5313 - val_loss: 0.6927 - val_acc: 0.5190\n",
      "Epoch 180/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5300 - val_loss: 0.6931 - val_acc: 0.5183\n",
      "Epoch 181/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6896 - acc: 0.5296 - val_loss: 0.6922 - val_acc: 0.5214\n",
      "Epoch 182/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6895 - acc: 0.5295 - val_loss: 0.6926 - val_acc: 0.5185\n",
      "Epoch 183/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6894 - acc: 0.5319 - val_loss: 0.6925 - val_acc: 0.5241\n",
      "Epoch 184/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6896 - acc: 0.5306 - val_loss: 0.6922 - val_acc: 0.5219\n",
      "Epoch 185/200\n",
      "152004/152004 [==============================] - 30s 194us/step - loss: 0.6894 - acc: 0.5313 - val_loss: 0.6926 - val_acc: 0.5197\n",
      "Epoch 186/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5319 - val_loss: 0.6926 - val_acc: 0.5166\n",
      "Epoch 187/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6895 - acc: 0.5306 - val_loss: 0.6933 - val_acc: 0.5145\n",
      "Epoch 188/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5306 - val_loss: 0.6928 - val_acc: 0.5195\n",
      "Epoch 189/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6894 - acc: 0.5305 - val_loss: 0.6926 - val_acc: 0.5215\n",
      "Epoch 190/200\n",
      "152004/152004 [==============================] - 29s 192us/step - loss: 0.6895 - acc: 0.5295 - val_loss: 0.6923 - val_acc: 0.5220\n",
      "Epoch 191/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6894 - acc: 0.5324 - val_loss: 0.6924 - val_acc: 0.5204\n",
      "Epoch 192/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6896 - acc: 0.5315 - val_loss: 0.6924 - val_acc: 0.5194\n",
      "Epoch 193/200\n",
      "152004/152004 [==============================] - 29s 191us/step - loss: 0.6895 - acc: 0.5307 - val_loss: 0.6922 - val_acc: 0.5226\n",
      "Epoch 194/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6893 - acc: 0.5325 - val_loss: 0.6930 - val_acc: 0.5170\n",
      "Epoch 195/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6894 - acc: 0.5309 - val_loss: 0.6928 - val_acc: 0.5181\n",
      "Epoch 196/200\n",
      "152004/152004 [==============================] - 29s 194us/step - loss: 0.6894 - acc: 0.5318 - val_loss: 0.6924 - val_acc: 0.5205\n",
      "Epoch 197/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6894 - acc: 0.5321 - val_loss: 0.6926 - val_acc: 0.5191\n",
      "Epoch 198/200\n",
      "152004/152004 [==============================] - 30s 195us/step - loss: 0.6893 - acc: 0.5323 - val_loss: 0.6935 - val_acc: 0.5152\n",
      "Epoch 199/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6895 - acc: 0.5319 - val_loss: 0.6923 - val_acc: 0.5199\n",
      "Epoch 200/200\n",
      "152004/152004 [==============================] - 29s 193us/step - loss: 0.6894 - acc: 0.5315 - val_loss: 0.6941 - val_acc: 0.5139\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Conv1D,Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "%matplotlib inline\n",
    "from keras import optimizers,initializers,regularizers\n",
    "model = Sequential() \n",
    "\n",
    "model.add(LSTM(units=16, return_sequences=True, \n",
    "               input_shape=(trainX.shape[1],trainX.shape[2])))#,kernel_regularizer=regularizers.l2(0.01)\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=16, return_sequences=True,activation='relu'))#,kernel_regularizer=regularizers.l2(0.01)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=16,return_sequences=False,activation='relu'))#,kernel_regularizer=regularizers.l2(0.01)\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(units = 1,activation='sigmoid'))#sigmoid\n",
    "model.compile(loss='binary_crossentropy',#binary_crossentropy\n",
    "                  optimizer='Adam',#RMSprop optimizers.Adam(lr=0.00001)\n",
    "                  metrics=['accuracy'])  #mean_squared_error metrics=['accuracy']\n",
    "history = model.fit(trainX,trainY,epochs=200,validation_split=0.33, batch_size=120, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5291049419489786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.22      0.32     37571\n",
      "           1       0.52      0.83      0.64     37966\n",
      "\n",
      "    accuracy                           0.53     75537\n",
      "   macro avg       0.54      0.53      0.48     75537\n",
      "weighted avg       0.54      0.53      0.48     75537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testPredict = model.predict_classes(testX)\n",
    "result(testY,testPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
