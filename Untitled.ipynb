{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tione/notebook/Autoencoder.py:6: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from Autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8729, 192)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('intecleaned-correct.csv',usecols=[1,2,4,5,6,7,8,9])\n",
    "dataset = dataframe.values\n",
    "#将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "def create_dataset(dataset, look_back,look_lag,data_step):\n",
    "    main_dataX, main_dataY ,auxi_dataX,auxi_dataY= [], [],[],[]\n",
    "    for i in range(0,len(dataset)-look_back-look_lag+1,data_step):\n",
    "        a = dataset[i:(i+look_back), 1:8] #前一天的短期预测湿度、短期预测风向、短期预测温度、\n",
    "                                        #短期预测气压、实际功率、实际风速都作为输入特征（共7个）\n",
    "        c = dataset[(i+look_back):(i+look_back+look_lag),1:5]#当天的短期预测功率、短期预测湿度、短期预测风向、短期预测温度、短期预测气压作为特征（共4个）\n",
    "        d = dataset[(i+look_back):(i+look_back+look_lag),7:8]#当天短期预测风速作为特征（共1个）\n",
    "        e = numpy.hstack([a,c])\n",
    "        e = numpy.hstack([d,e])#把上面的特征都合在一起，共12个特征\n",
    "        main_dataX.append(e) #主要输入\n",
    "        b = dataset[(i+look_back):(i+look_back+look_lag),5]\n",
    "        main_dataY.append(b) #主要输出 需要预测的功率数据\n",
    "    return numpy.array(main_dataX), numpy.array(main_dataY)#,numpy.array(auxi_dataX),numpy.array(auxi_dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# normalize the dataset\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))#功率数据归一化，之后还原预测结果要用\n",
    "Y = scaler_y.fit_transform(dataset[:,5:6])\n",
    "\n",
    "scaler_y1 = MinMaxScaler(feature_range=(0, 1))#风速数据归一化\n",
    "Y1 = scaler_y1.fit_transform(dataset[:,6:7])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "look_back = 16\n",
    "look_lag = 16\n",
    "train_size = 364*96\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size-look_back:len(dataset),:]\n",
    "train_mainX, train_mainY = create_dataset(train, look_back,look_lag,4)#,train_auxiX,train_auxiY\n",
    "test_mainX, test_mainY = create_dataset(test, look_back,look_lag,16)#,test_auxiX,test_auxiY\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "train_mainX = numpy.reshape(train_mainX, (train_mainX.shape[0], look_back*12))\n",
    "\n",
    "test_mainX = numpy.reshape(test_mainX, (test_mainX.shape[0], look_back*12))\n",
    "train_mainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 713.003268559\n",
      "Epoch: 2, Cost: 296.409362380\n",
      "Epoch: 3, Cost: 241.675505812\n",
      "Epoch: 4, Cost: 212.928987718\n",
      "Epoch: 5, Cost: 198.849948806\n",
      "Epoch: 6, Cost: 194.094523061\n",
      "Epoch: 7, Cost: 180.529019871\n",
      "Epoch: 8, Cost: 185.279947991\n",
      "Epoch: 9, Cost: 175.814925911\n",
      "Epoch: 10, Cost: 166.484022032\n",
      "Epoch: 11, Cost: 165.072690752\n",
      "Epoch: 12, Cost: 174.189052722\n",
      "Epoch: 13, Cost: 164.341564397\n",
      "Epoch: 14, Cost: 160.082255617\n",
      "Epoch: 15, Cost: 143.865258428\n",
      "Epoch: 16, Cost: 159.914332456\n",
      "Epoch: 17, Cost: 156.181059431\n",
      "Epoch: 18, Cost: 156.709754695\n",
      "Epoch: 19, Cost: 163.746492810\n",
      "Epoch: 20, Cost: 162.775584170\n",
      "Epoch: 21, Cost: 139.117037699\n",
      "Epoch: 22, Cost: 150.658308736\n",
      "Epoch: 23, Cost: 149.630433856\n",
      "Epoch: 24, Cost: 125.894695651\n",
      "Epoch: 25, Cost: 133.612409739\n",
      "Epoch: 26, Cost: 129.396651573\n",
      "Epoch: 27, Cost: 135.611359003\n",
      "Epoch: 28, Cost: 127.414519792\n",
      "Epoch: 29, Cost: 110.597791955\n",
      "Epoch: 30, Cost: 108.525928411\n",
      "Epoch: 31, Cost: 105.709594181\n",
      "Epoch: 32, Cost: 108.006375094\n",
      "Epoch: 33, Cost: 105.977877710\n",
      "Epoch: 34, Cost: 106.652757802\n",
      "Epoch: 35, Cost: 101.990162075\n",
      "Epoch: 36, Cost: 99.548745561\n",
      "Epoch: 37, Cost: 97.267266160\n",
      "Epoch: 38, Cost: 93.794408813\n",
      "Epoch: 39, Cost: 85.285849515\n",
      "Epoch: 40, Cost: 87.901781715\n",
      "Epoch: 41, Cost: 91.337851189\n",
      "Epoch: 42, Cost: 94.238447078\n",
      "Epoch: 43, Cost: 96.951086402\n",
      "Epoch: 44, Cost: 92.853900953\n",
      "Epoch: 45, Cost: 93.671874329\n",
      "Epoch: 46, Cost: 94.797924533\n",
      "Epoch: 47, Cost: 92.831709048\n",
      "Epoch: 48, Cost: 93.268155403\n",
      "Epoch: 49, Cost: 94.247758516\n",
      "Epoch: 50, Cost: 95.670680392\n",
      "Epoch: 51, Cost: 96.653253635\n",
      "Epoch: 52, Cost: 93.746573081\n",
      "Epoch: 53, Cost: 94.805118221\n",
      "Epoch: 54, Cost: 96.242530119\n",
      "Epoch: 55, Cost: 95.834975761\n",
      "Epoch: 56, Cost: 93.808956143\n",
      "Epoch: 57, Cost: 93.681282875\n",
      "Epoch: 58, Cost: 85.848033830\n",
      "Epoch: 59, Cost: 85.730242286\n",
      "Epoch: 60, Cost: 85.979103923\n",
      "Epoch: 61, Cost: 84.664890799\n",
      "Epoch: 62, Cost: 84.582529712\n",
      "Epoch: 63, Cost: 81.519619016\n",
      "Epoch: 64, Cost: 82.363056548\n",
      "Epoch: 65, Cost: 83.531525885\n",
      "Epoch: 66, Cost: 86.413582637\n",
      "Epoch: 67, Cost: 85.738357717\n",
      "Epoch: 68, Cost: 88.266756790\n",
      "Epoch: 69, Cost: 88.945781874\n",
      "Epoch: 70, Cost: 88.653004879\n",
      "Epoch: 71, Cost: 88.481370887\n",
      "Epoch: 72, Cost: 86.538743319\n",
      "Epoch: 73, Cost: 86.482250901\n",
      "Epoch: 74, Cost: 87.054083008\n",
      "Epoch: 75, Cost: 86.645383351\n",
      "Epoch: 76, Cost: 86.465018891\n",
      "Epoch: 77, Cost: 84.061416820\n",
      "Epoch: 78, Cost: 84.676414830\n",
      "Epoch: 79, Cost: 84.435526038\n",
      "Epoch: 80, Cost: 81.853698374\n",
      "Epoch: 81, Cost: 80.658648194\n",
      "Epoch: 82, Cost: 81.697159511\n",
      "Epoch: 83, Cost: 80.451239280\n",
      "Epoch: 84, Cost: 82.617541195\n",
      "Epoch: 85, Cost: 83.443127009\n",
      "Epoch: 86, Cost: 85.238426101\n",
      "Epoch: 87, Cost: 82.042563760\n",
      "Epoch: 88, Cost: 82.996020220\n",
      "Epoch: 89, Cost: 87.446723224\n",
      "Epoch: 90, Cost: 86.102080177\n",
      "Epoch: 91, Cost: 86.339403659\n",
      "Epoch: 92, Cost: 87.576069565\n",
      "Epoch: 93, Cost: 91.188379818\n",
      "Epoch: 94, Cost: 88.398206589\n",
      "Epoch: 95, Cost: 87.221760294\n",
      "Epoch: 96, Cost: 83.655053378\n",
      "Epoch: 97, Cost: 83.811753398\n",
      "Epoch: 98, Cost: 86.325063439\n",
      "Epoch: 99, Cost: 83.990922998\n",
      "Epoch: 100, Cost: 82.341796344\n",
      "************************First AE training finished******************************\n",
      "Epoch: 1, Cost: 763.992402077\n",
      "Epoch: 2, Cost: 423.242185039\n",
      "Epoch: 3, Cost: 289.462028847\n",
      "Epoch: 4, Cost: 232.079191734\n",
      "Epoch: 5, Cost: 206.357674602\n",
      "Epoch: 6, Cost: 194.257558095\n",
      "Epoch: 7, Cost: 179.471015258\n",
      "Epoch: 8, Cost: 173.813389299\n",
      "Epoch: 9, Cost: 168.147342014\n",
      "Epoch: 10, Cost: 156.719797436\n",
      "Epoch: 11, Cost: 157.279330438\n",
      "Epoch: 12, Cost: 155.866803990\n",
      "Epoch: 13, Cost: 146.565627763\n",
      "Epoch: 14, Cost: 144.339169587\n",
      "Epoch: 15, Cost: 141.149514862\n",
      "Epoch: 16, Cost: 137.792796517\n",
      "Epoch: 17, Cost: 131.607736637\n",
      "Epoch: 18, Cost: 131.134500032\n",
      "Epoch: 19, Cost: 129.905314831\n",
      "Epoch: 20, Cost: 132.189064022\n",
      "Epoch: 21, Cost: 128.400126710\n",
      "Epoch: 22, Cost: 125.031120168\n",
      "Epoch: 23, Cost: 119.455770691\n",
      "Epoch: 24, Cost: 123.322665804\n",
      "Epoch: 25, Cost: 119.263312754\n",
      "Epoch: 26, Cost: 119.316674416\n",
      "Epoch: 27, Cost: 117.835116277\n",
      "Epoch: 28, Cost: 114.855275205\n",
      "Epoch: 29, Cost: 117.041684585\n",
      "Epoch: 30, Cost: 115.195543915\n",
      "Epoch: 31, Cost: 111.253068022\n",
      "Epoch: 32, Cost: 113.256086986\n",
      "Epoch: 33, Cost: 112.337156889\n",
      "Epoch: 34, Cost: 111.690486744\n",
      "Epoch: 35, Cost: 112.576845814\n",
      "Epoch: 36, Cost: 110.508630591\n",
      "Epoch: 37, Cost: 109.178982124\n",
      "Epoch: 38, Cost: 107.458130204\n",
      "Epoch: 39, Cost: 108.951604722\n",
      "Epoch: 40, Cost: 106.932497425\n",
      "Epoch: 41, Cost: 108.387336527\n",
      "Epoch: 42, Cost: 105.580271110\n",
      "Epoch: 43, Cost: 104.653500153\n",
      "Epoch: 44, Cost: 106.048071007\n",
      "Epoch: 45, Cost: 105.113343788\n",
      "Epoch: 46, Cost: 104.757153273\n",
      "Epoch: 47, Cost: 104.269352473\n",
      "Epoch: 48, Cost: 103.369834534\n",
      "Epoch: 49, Cost: 103.016985632\n",
      "Epoch: 50, Cost: 102.030390403\n",
      "Epoch: 51, Cost: 103.148940717\n",
      "Epoch: 52, Cost: 101.699044392\n",
      "Epoch: 53, Cost: 99.883644239\n",
      "Epoch: 54, Cost: 98.633796055\n",
      "Epoch: 55, Cost: 101.563389859\n",
      "Epoch: 56, Cost: 100.076111015\n",
      "Epoch: 57, Cost: 97.560167337\n",
      "Epoch: 58, Cost: 98.743798286\n",
      "Epoch: 59, Cost: 99.999070425\n",
      "Epoch: 60, Cost: 96.991112263\n",
      "Epoch: 61, Cost: 98.829727003\n",
      "Epoch: 62, Cost: 95.446262828\n",
      "Epoch: 63, Cost: 95.433405016\n",
      "Epoch: 64, Cost: 94.799634637\n",
      "Epoch: 65, Cost: 95.517287976\n",
      "Epoch: 66, Cost: 96.363511770\n",
      "Epoch: 67, Cost: 94.291088064\n",
      "Epoch: 68, Cost: 93.212008486\n",
      "Epoch: 69, Cost: 92.808019213\n",
      "Epoch: 70, Cost: 93.067318372\n",
      "Epoch: 71, Cost: 94.132280419\n",
      "Epoch: 72, Cost: 93.547352541\n",
      "Epoch: 73, Cost: 93.295285751\n",
      "Epoch: 74, Cost: 93.946795468\n",
      "Epoch: 75, Cost: 92.899907210\n",
      "Epoch: 76, Cost: 93.678109326\n",
      "Epoch: 77, Cost: 92.101453701\n",
      "Epoch: 78, Cost: 91.447571213\n",
      "Epoch: 79, Cost: 90.888649556\n",
      "Epoch: 80, Cost: 91.612084292\n",
      "Epoch: 81, Cost: 91.698956178\n",
      "Epoch: 82, Cost: 90.572874173\n",
      "Epoch: 83, Cost: 90.792966121\n",
      "Epoch: 84, Cost: 89.819811577\n",
      "Epoch: 85, Cost: 91.332394286\n",
      "Epoch: 86, Cost: 88.104514786\n",
      "Epoch: 87, Cost: 88.653338297\n",
      "Epoch: 88, Cost: 88.199414476\n",
      "Epoch: 89, Cost: 86.483088822\n",
      "Epoch: 90, Cost: 88.725351558\n",
      "Epoch: 91, Cost: 87.262582038\n",
      "Epoch: 92, Cost: 85.834875129\n",
      "Epoch: 93, Cost: 89.145916243\n",
      "Epoch: 94, Cost: 88.083343160\n",
      "Epoch: 95, Cost: 89.072381304\n",
      "Epoch: 96, Cost: 87.416820906\n",
      "Epoch: 97, Cost: 89.235988387\n",
      "Epoch: 98, Cost: 88.328123042\n",
      "Epoch: 99, Cost: 89.441721180\n",
      "Epoch: 100, Cost: 86.079710390\n",
      "************************Second AE training finished******************************\n",
      "*************************Finish the softmax output layer training*****************************\n"
     ]
    }
   ],
   "source": [
    "n_samples = train_mainX.shape[0]\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "corruption_level = 0.3\n",
    "sparse_reg = 0\n",
    "\n",
    "#\n",
    "n_inputs =192\n",
    "n_hidden = 400\n",
    "n_hidden2 = 100\n",
    "n_outputs = 16\n",
    "lr = 0.001\n",
    "\n",
    "# define the autoencoder\n",
    "ae = Autoencoder(n_layers=[n_inputs, n_hidden],\n",
    "                          transfer_function = tf.nn.relu,\n",
    "                          optimizer = tf.train.AdamOptimizer(learning_rate = lr),\n",
    "                          ae_para = [corruption_level, sparse_reg])\n",
    "ae_2nd = Autoencoder(n_layers=[n_hidden, n_hidden2],\n",
    "                          transfer_function = tf.nn.relu,\n",
    "                          optimizer = tf.train.AdamOptimizer(learning_rate = lr),\n",
    "                          ae_para=[corruption_level, sparse_reg])\n",
    "# define the output layer using softmax\n",
    "x = tf.placeholder(tf.float32, [None, n_hidden2])\n",
    "W = tf.Variable(tf.zeros([n_hidden2, n_outputs]))\n",
    "b = tf.Variable(tf.zeros([n_outputs]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, n_outputs])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "train_step  = tf.train.AdamOptimizer(learning_rate = lr).minimize(cross_entropy)\n",
    "\n",
    "## define the output layer using softmax in the fine tuning step\n",
    "x_ft = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "h = x_ft\n",
    "\n",
    "# Go through the two autoencoders\n",
    "for layer in range(len(ae.n_layers) - 1):\n",
    "    # h = tf.nn.dropout(h, ae.in_keep_prob)\n",
    "    h = ae.transfer(\n",
    "        tf.add(tf.matmul(h, ae.weights['encode'][layer]['w']),ae.weights['encode'][layer]['b']))\n",
    "for layer in range(len(ae_2nd.n_layers) - 1):\n",
    "    # h = tf.nn.dropout(h, ae_2nd.in_keep_prob)\n",
    "    h = ae_2nd.transfer(\n",
    "        tf.add(tf.matmul(h, ae_2nd.weights['encode'][layer]['w']),ae_2nd.weights['encode'][layer]['b']))\n",
    "\n",
    "y_ft = tf.matmul(h, W) + b\n",
    "cross_entropy_ft = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_ft, labels=y_))\n",
    "\n",
    "train_step_ft  = tf.train.AdamOptimizer(learning_rate = lr).minimize(cross_entropy_ft)\n",
    "correct_prediction = tf.equal(tf.argmax(y_ft, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "       # batch_xs,_ = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = train_mainX[i*batch_size:i*batch_size+batch_size,:]\n",
    "        # Fit training using batch data\n",
    "        temp = ae.partial_fit()\n",
    "        cost, opt = sess.run(temp,feed_dict={ae.x: batch_xs, ae.keep_prob : ae.in_keep_prob})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%d,' % (epoch + 1),\n",
    "              \"Cost:\", \"{:.9f}\".format(avg_cost))\n",
    "#ae_test_cost = sess.run(ae.calc_total_cost(),feed_dict={ae.x: mnist.test.images, ae.keep_prob : 1.0})\n",
    "#print(\"Total cost: \" + str(ae_test_cost))\n",
    "\n",
    "print(\"************************First AE training finished******************************\")\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "      #  batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = train_mainX[i*batch_size:i*batch_size+batch_size,:]\n",
    "        # Fit training using batch data\n",
    "        h_ae1_out = sess.run(ae.transform(),feed_dict={ae.x: batch_xs, ae.keep_prob : ae.in_keep_prob})\n",
    "        temp = ae_2nd.partial_fit()\n",
    "        cost, opt = sess.run(temp,feed_dict={ae_2nd.x: h_ae1_out, ae_2nd.keep_prob : ae_2nd.in_keep_prob})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%d,' % (epoch + 1),\n",
    "              \"Cost:\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"************************Second AE training finished******************************\")\n",
    "\n",
    "\n",
    "# Training the softmax layer\n",
    "for i in range(1000):\n",
    "   # batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    batch_xs = train_mainX[i*batch_size:i*batch_size+batch_size,:]\n",
    "    batch_ys = train_mainY[i*batch_size:i*batch_size+batch_size,:]\n",
    "    h_ae1_out = sess.run(ae.transform(), feed_dict={ae.x: batch_xs, ae.keep_prob : 1.0})\n",
    "    h_ae2_out = sess.run(ae_2nd.transform(), feed_dict={ae_2nd.x: h_ae1_out, ae_2nd.keep_prob : 1.0})\n",
    "    sess.run(train_step, feed_dict={x: h_ae2_out, y_: batch_ys})\n",
    "print(\"*************************Finish the softmax output layer training*****************************\")\n",
    "\n",
    "#print(\"Test accuracy before fine-tune\")\n",
    "#print(sess.run(accuracy, feed_dict={x_ft: mnist.test.images, y_: mnist.test.labels,\n",
    "#                                    ae.keep_prob: 1.0,ae_2nd.keep_prob : 1.0}))\n",
    "\n",
    "# Training of fine tune\n",
    "for i in range(1000):\n",
    "  #  batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    batch_xs = train_mainX[i*batch_size:i*batch_size+batch_size,:]\n",
    "    batch_ys = train_mainY[i*batch_size:i*batch_size+batch_size,:]\n",
    "    sess.run(train_step_ft,feed_dict={x_ft: batch_xs, y_: batch_ys,\n",
    "                                      ae.keep_prob: 1.0, ae_2nd.keep_prob: 1.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
